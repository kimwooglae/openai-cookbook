{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# 틱토큰으로 토큰을 계산하는 방법\n",
            "\n",
            "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md)은 OpenAI의 빠른 오픈소스 토큰 생성기입니다.\n",
            "\n",
            "토큰화기는 텍스트 문자열(예: `\"tiktoken is great!\"`)과 인코딩(예: `\"cl100k_base\"`)이 주어지면 텍스트 문자열을 토큰 목록(예: `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`)으로 분할할 수 있습니다.\n",
            "\n",
            "텍스트 문자열을 토큰으로 분할하는 것은 GPT 모델이 텍스트를 토큰 형태로 보기 때문에 유용합니다. 텍스트 문자열에 몇 개의 토큰이 있는지 알면 (1) 텍스트 모델이 처리하기에 문자열이 너무 긴지 여부와 (2) OpenAI API 호출 비용이 얼마인지 알 수 있습니다(사용량은 토큰별로 가격이 책정되므로). 모델마다 다른 인코딩을 사용합니다.\n",
            "\n",
            "---\n",
            "\n",
            "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md) is a fast open-source tokenizer by OpenAI.\n",
            "\n",
            "Given a text string (e.g., `\"tiktoken is great!\"`) and an encoding (e.g., `\"cl100k_base\"`), a tokenizer can split the text string into a list of tokens (e.g., `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`).\n",
            "\n",
            "Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token).\n",
            "\n",
            "\n",
            "## 인코딩\n",
            "\n",
            "인코딩은 텍스트가 토큰으로 변환되는 방식을 지정합니다. 모델마다 다른 인코딩을 사용합니다.\n",
            "\n",
            "`tiktoken`은 OpenAI 모델에서 사용하는 세 가지 인코딩을 지원합니다:\n",
            "\n",
            "| 인코딩 이름 | OpenAI 모델 |\n",
            "|-------------------------|-----------------------------------------------------|\n",
            "| `cl100k_base`           | ChatGPT models, `text-embedding-ada-002`            |\n",
            "| `p50k_base`             | Code models, `text-davinci-002`, `text-davinci-003` |\n",
            "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
            "\n",
            "틱토큰의 모델 인코딩은 다음과 같이 `tiktoken.encoding_for_model()`을 사용하여 검색할 수 있습니다:\n",
            "```python\n",
            "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
            "```\n",
            "\n",
            "`p50k_base`는 `r50k_base`와 상당 부분 겹치며, 코드가 아닌 애플리케이션의 경우 일반적으로 동일한 토큰을 제공합니다.\n",
            "\n",
            "---\n",
            "\n",
            "Encodings specify how text is converted into tokens. Different models use different encodings.\n",
            "\n",
            "`tiktoken` supports three encodings used by OpenAI models:\n",
            "\n",
            "| Encoding name           | OpenAI models                                       |\n",
            "|-------------------------|-----------------------------------------------------|\n",
            "| `cl100k_base`           | `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`  |\n",
            "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\n",
            "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
            "\n",
            "You can retrieve the encoding for a model using `tiktoken.encoding_for_model()` as follows:\n",
            "```python\n",
            "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
            "```\n",
            "\n",
            "Note that `p50k_base` overlaps substantially with `r50k_base`, and for non-code applications, they will usually give the same tokens.\n",
            "\n",
            "## 언어별 토큰화 라이브러리\n",
            "\n",
            "`cl100k_base` 및 `p50k_base` 인코딩의 경우, 2023년 3월 현재 사용 가능한 토큰라이저는 `tiktoken`뿐입니다.\n",
            "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
            "\n",
            "`r50k_base`(`gpt2`) 인코딩의 경우, 토큰화 도구는 여러 언어로 제공됩니다.\n",
            "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (또는 [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
            "- 자바스크립트: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
            "- .NET/C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
            "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
            "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
            "\n",
            "(OpenAI는 타사 라이브러리를 보증하거나 보증하지 않습니다.)\n",
            "\n",
            "---\n",
            "\n",
            "For `cl100k_base` and `p50k_base` encodings:\n",
            "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
            "- .NET / C#: [SharpToken](https://github.com/dmitry-brazhenko/SharpToken)\n",
            "\n",
            "For `r50k_base` (`gpt2`) encodings, tokenizers are available in many languages.\n",
            "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (or alternatively [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
            "- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
            "- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
            "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
            "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
            "\n",
            "(OpenAI makes no endorsements or guarantees of third-party libraries.)\n",
            "\n",
            "\n",
            "## 문자열이 일반적으로 토큰화되는 방식\n",
            "\n",
            "영어에서 토큰의 길이는 일반적으로 한 문자에서 한 단어(예: `\"t\"` 또는 `\"great\"`)까지이지만, 일부 언어에서는 토큰이 한 문자보다 짧거나 한 단어보다 길 수 있습니다. 공백은 일반적으로 단어의 시작 부분으로 그룹화됩니다(예: `\"is \"` 또는 `\" \"`+`\"is\"` 대신 `\" is\"`). 문자열이 토큰화되는 방식은 [OpenAI Tokenizer](https://beta.openai.com/tokenizer)에서 빠르게 확인할 수 있습니다.\n",
            "\n",
            "---\n",
            "\n",
            "In English, tokens commonly range in length from one character to one word (e.g., `\"t\"` or `\" great\"`), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., `\" is\"` instead of `\"is \"` or `\" \"`+`\"is\"`). You can quickly check how a string is tokenized at the [OpenAI Tokenizer](https://beta.openai.com/tokenizer)."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 0. Install `tiktoken`\n",
            "\n",
            "If needed, install `tiktoken` with `pip`:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/site-packages (0.2.0)\n",
                  "Collecting tiktoken\n",
                  "  Downloading tiktoken-0.3.0-cp310-cp310-macosx_10_9_x86_64.whl (735 kB)\n",
                  "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.2/735.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
                  "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2022.10.31)\n",
                  "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2.0.1)\n",
                  "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2.28.2)\n",
                  "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (1.26.14)\n",
                  "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
                  "Requirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (3.9.0)\n",
                  "Requirement already satisfied: pycryptodomex~=3.8 in /usr/local/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (3.17)\n",
                  "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
                  "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
                  "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.0.1)\n",
                  "Installing collected packages: tiktoken\n",
                  "  Attempting uninstall: tiktoken\n",
                  "    Found existing installation: tiktoken 0.2.0\n",
                  "    Uninstalling tiktoken-0.2.0:\n",
                  "      Successfully uninstalled tiktoken-0.2.0\n",
                  "Successfully installed tiktoken-0.3.0\n",
                  "\n",
                  "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
                  "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
                  "Note: you may need to restart the kernel to use updated packages.\n"
               ]
            }
         ],
         "source": [
            "%pip install --upgrade tiktoken"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 1. Import `tiktoken`"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "import tiktoken\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 2. 인코딩 로드하기\n",
            "\n",
            "`tiktoken.get_encoding()`을 사용하여 이름별로 인코딩을 로드합니다.\n",
            "\n",
            "처음 실행할 때 다운로드하려면 인터넷 연결이 필요합니다. 나중에 실행할 때는 인터넷 연결이 필요하지 않습니다.\n",
            "\n",
            "---\n",
            "\n",
            "Use `tiktoken.get_encoding()` to load an encoding by name.\n",
            "\n",
            "The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "encoding = tiktoken.get_encoding(\"cl100k_base\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "주어진 모델 이름에 맞는 올바른 인코딩을 자동으로 로드하려면 `tiktoken.encoding_for_model()`을 사용하세요.\n",
            "\n",
            "---\n",
            "\n",
            "Use `tiktoken.encoding_for_model()` to automatically load the correct encoding for a given model name."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "# encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
            "# encoding = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
            "encoding = tiktoken.encoding_for_model(\"text-embedding-ada-002\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "<Encoding 'cl100k_base'>"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "encoding"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 3. `encoding.encode()`로 텍스트를 토큰으로 변환하기\n",
            "\n",
            "`.encode()` 메서드는 텍스트 문자열을 토큰 정수의 목록으로 변환합니다.\n",
            "\n",
            "---\n",
            "\n",
            "The `.encode()` method converts a text string into a list of token integers."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 20,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[169,\n",
                     " 233,\n",
                     " 109,\n",
                     " 169,\n",
                     " 228,\n",
                     " 58260,\n",
                     " 223,\n",
                     " 108,\n",
                     " 34804,\n",
                     " 10997,\n",
                     " 249,\n",
                     " 234,\n",
                     " 16306,\n",
                     " 255,\n",
                     " 61938,\n",
                     " 0]"
                  ]
               },
               "execution_count": 20,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# encoding.encode(\"tiktoken is great!\")\n",
            "encoding.encode(\"틱토큰은 훌륭합니다!\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "`.encode()`가 반환한 목록의 길이를 계산하여 토큰 수를 계산합니다.\n",
            "\n",
            "---\n",
            "\n",
            "Count tokens by counting the length of the list returned by `.encode()`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 21,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
            "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
            "    encoding = tiktoken.get_encoding(encoding_name)\n",
            "    num_tokens = len(encoding.encode(string))\n",
            "    return num_tokens\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 22,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "16"
                  ]
               },
               "execution_count": 22,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n",
            "num_tokens_from_string(\"틱토큰은 훌륭합니다!\", \"cl100k_base\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 4. `encoding.decode()`로 토큰을 텍스트로 변환하기\n",
            "\n",
            "`.decode()`는 토큰 정수 목록을 문자열로 변환합니다.\n",
            "\n",
            "---\n",
            "\n",
            "`.decode()` converts a list of token integers to a string."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 24,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "'틱토큰은 훌륭합니다!'"
                  ]
               },
               "execution_count": 24,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# encoding.decode([83, 1609, 5963, 374, 2294, 0])\n",
            "encoding.decode([169, 233, 109, 169, 228, 58260, 223, 108, 34804, 10997, 249, 234, 16306, 255, 61938, 0])"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "경고: `.decode()`는 단일 토큰에 적용될 수 있지만, utf-8 경계에 있지 않은 토큰의 경우 손실이 발생할 수 있으니 주의하세요.\n",
            "\n",
            "단일 토큰의 경우 `.decode_single_token_bytes()`는 단일 정수 토큰을 해당 토큰이 나타내는 바이트 수로 안전하게 변환합니다.\n",
            "\n",
            "---\n",
            "\n",
            "Warning: although `.decode()` can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries.\n",
            "\n",
            "For single tokens, `.decode_single_token_bytes()` safely converts a single integer token to the bytes it represents."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 33,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "data": {
                  "text/plain": [
                     "[b'\\xed',\n",
                     " b'\\x8b',\n",
                     " b'\\xb1',\n",
                     " b'\\xed',\n",
                     " b'\\x86',\n",
                     " b'\\xa0\\xed',\n",
                     " b'\\x81',\n",
                     " b'\\xb0',\n",
                     " b'\\xec\\x9d\\x80',\n",
                     " b' \\xed',\n",
                     " b'\\x9b',\n",
                     " b'\\x8c',\n",
                     " b'\\xeb\\xa5',\n",
                     " b'\\xad',\n",
                     " b'\\xed\\x95\\xa9\\xeb\\x8b\\x88\\xeb\\x8b\\xa4',\n",
                     " b'!']"
                  ]
               },
               "execution_count": 33,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "# [encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]\n",
            "[encoding.decode_single_token_bytes(token) for token in [169, 233, 109, 169, 228, 58260, 223, 108, 34804, 10997, 249, 234, 16306, 255, 61938, 0]]\n",
            "# [encoding.decode([token]) for token in [169, 233, 109, 169, 228, 58260, 223, 108, 34804, 10997, 249, 234, 16306, 255, 61938, 0]]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "(문자열 앞의 `b`는 문자열이 바이트 문자열임을 나타냅니다.)\n",
            "\n",
            "(The `b` in front of the strings indicates that the strings are byte strings.)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 5. 인코딩 비교하기\n",
            "\n",
            "인코딩에 따라 단어를 분할하고, 공백을 그룹화하고, 영어가 아닌 문자를 처리하는 방식이 다를 수 있습니다. 위의 방법을 사용하여 몇 가지 예제 문자열에서 서로 다른 인코딩을 비교할 수 있습니다.\n",
            "\n",
            "Different encodings vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 34,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "def compare_encodings(example_string: str) -> None:\n",
            "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
            "    # print the example string\n",
            "    print(f'\\nExample string: \"{example_string}\"')\n",
            "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
            "    for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:\n",
            "        encoding = tiktoken.get_encoding(encoding_name)\n",
            "        token_integers = encoding.encode(example_string)\n",
            "        num_tokens = len(token_integers)\n",
            "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
            "        print()\n",
            "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
            "        print(f\"token integers: {token_integers}\")\n",
            "        print(f\"token bytes: {token_bytes}\")\n",
            "        "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 36,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "Example string: \"틱토큰은 훌륭합니다!\"\n",
                  "\n",
                  "gpt2: 25 tokens\n",
                  "token integers: [169, 233, 109, 169, 228, 254, 169, 223, 108, 35975, 222, 220, 169, 249, 234, 167, 98, 255, 47991, 102, 46695, 230, 46695, 97, 0]\n",
                  "token bytes: [b'\\xed', b'\\x8b', b'\\xb1', b'\\xed', b'\\x86', b'\\xa0', b'\\xed', b'\\x81', b'\\xb0', b'\\xec\\x9d', b'\\x80', b' ', b'\\xed', b'\\x9b', b'\\x8c', b'\\xeb', b'\\xa5', b'\\xad', b'\\xed\\x95', b'\\xa9', b'\\xeb\\x8b', b'\\x88', b'\\xeb\\x8b', b'\\xa4', b'!']\n",
                  "\n",
                  "p50k_base: 25 tokens\n",
                  "token integers: [169, 233, 109, 169, 228, 254, 169, 223, 108, 35975, 222, 220, 169, 249, 234, 167, 98, 255, 47991, 102, 46695, 230, 46695, 97, 0]\n",
                  "token bytes: [b'\\xed', b'\\x8b', b'\\xb1', b'\\xed', b'\\x86', b'\\xa0', b'\\xed', b'\\x81', b'\\xb0', b'\\xec\\x9d', b'\\x80', b' ', b'\\xed', b'\\x9b', b'\\x8c', b'\\xeb', b'\\xa5', b'\\xad', b'\\xed\\x95', b'\\xa9', b'\\xeb\\x8b', b'\\x88', b'\\xeb\\x8b', b'\\xa4', b'!']\n",
                  "\n",
                  "cl100k_base: 16 tokens\n",
                  "token integers: [169, 233, 109, 169, 228, 58260, 223, 108, 34804, 10997, 249, 234, 16306, 255, 61938, 0]\n",
                  "token bytes: [b'\\xed', b'\\x8b', b'\\xb1', b'\\xed', b'\\x86', b'\\xa0\\xed', b'\\x81', b'\\xb0', b'\\xec\\x9d\\x80', b' \\xed', b'\\x9b', b'\\x8c', b'\\xeb\\xa5', b'\\xad', b'\\xed\\x95\\xa9\\xeb\\x8b\\x88\\xeb\\x8b\\xa4', b'!']\n"
               ]
            }
         ],
         "source": [
            "# compare_encodings(\"antidisestablishmentarianism\")\n",
            "compare_encodings(\"틱토큰은 훌륭합니다!\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 37,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "Example string: \"2 + 2 = 4\"\n",
                  "\n",
                  "gpt2: 5 tokens\n",
                  "token integers: [17, 1343, 362, 796, 604]\n",
                  "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
                  "\n",
                  "p50k_base: 5 tokens\n",
                  "token integers: [17, 1343, 362, 796, 604]\n",
                  "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
                  "\n",
                  "cl100k_base: 7 tokens\n",
                  "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
                  "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
               ]
            }
         ],
         "source": [
            "compare_encodings(\"2 + 2 = 4\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 38,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "\n",
                  "Example string: \"お誕生日おめでとう\"\n",
                  "\n",
                  "gpt2: 14 tokens\n",
                  "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
                  "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
                  "\n",
                  "p50k_base: 14 tokens\n",
                  "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
                  "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
                  "\n",
                  "cl100k_base: 9 tokens\n",
                  "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
                  "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
               ]
            }
         ],
         "source": [
            "compare_encodings(\"お誕生日おめでとう\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## 6. 채팅 API 호출에 대한 토큰 카운팅\n",
            "\n",
            "`gpt-3.5-turbo`와 같은 ChatGPT 모델은 다른 모델과 같은 방식으로 토큰을 사용하지만, 메시지 기반 형식이기 때문에 대화에 사용되는 토큰의 수를 계산하기가 더 어렵습니다.\n",
            "\n",
            "아래는 `gpt-3.5-turbo-0301`에 전달된 메시지의 토큰을 계산하는 예제 함수입니다.\n",
            "\n",
            "메시지가 토큰으로 변환되는 정확한 방식은 모델마다 다를 수 있습니다. 따라서 향후 모델 버전이 출시되면 이 함수가 반환하는 답변은 근사치에 불과할 수 있습니다. [ChatML 문서](https://github.com/openai/openai-python/blob/main/chatml.md)에서 OpenAI API가 메시지를 토큰으로 변환하는 방법을 설명하고 있으며, 자체 함수를 작성하는 데 유용할 수 있습니다.\n",
            "\n",
            "---\n",
            "\n",
            "ChatGPT models like `gpt-3.5-turbo` and `gpt-4` use tokens in the same way as older completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n",
            "\n",
            "Below is an example function for counting tokens for messages passed to `gpt-3.5-turbo-0301` or `gpt-4-0314`.\n",
            "\n",
            "Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 39,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
            "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
            "    try:\n",
            "        encoding = tiktoken.encoding_for_model(model)\n",
            "    except KeyError:\n",
            "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
            "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
            "    if model == \"gpt-3.5-turbo\":\n",
            "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
            "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
            "    elif model == \"gpt-4\":\n",
            "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
            "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
            "    elif model == \"gpt-3.5-turbo-0301\":\n",
            "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
            "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
            "    elif model == \"gpt-4-0314\":\n",
            "        tokens_per_message = 3\n",
            "        tokens_per_name = 1\n",
            "    else:\n",
            "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
            "    num_tokens = 0\n",
            "    for message in messages:\n",
            "        num_tokens += tokens_per_message\n",
            "        for key, value in message.items():\n",
            "            num_tokens += len(encoding.encode(value))\n",
            "            if key == \"name\":\n",
            "                num_tokens += tokens_per_name\n",
            "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
            "    return num_tokens\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 40,
         "metadata": {
            "tags": []
         },
         "outputs": [],
         "source": [
            "messages = [\n",
            "    {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
            "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
            "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
            "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
            "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
            "    {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
            "]\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 41,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "gpt-3.5-turbo-0301\n",
                  "127 prompt tokens counted by num_tokens_from_messages().\n",
                  "127 prompt tokens counted by the OpenAI API.\n",
                  "\n",
                  "gpt-4-0314\n",
                  "129 prompt tokens counted by num_tokens_from_messages().\n",
                  "129 prompt tokens counted by the OpenAI API.\n",
                  "\n"
               ]
            }
         ],
         "source": [
            "# let's verify the function above matches the OpenAI API response\n",
            "\n",
            "print(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 42,
         "metadata": {
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "126 prompt tokens used.\n"
               ]
            }
         ],
         "source": [
            "# example token count from the OpenAI API\n",
            "import openai\n",
            "\n",
            "example_messages = [\n",
            "    {\n",
            "        \"role\": \"system\",\n",
            "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"system\",\n",
            "        \"name\": \"example_user\",\n",
            "        \"content\": \"New synergies will help drive top-line growth.\",\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"system\",\n",
            "        \"name\": \"example_assistant\",\n",
            "        \"content\": \"Things working well together will increase revenue.\",\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"system\",\n",
            "        \"name\": \"example_user\",\n",
            "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"system\",\n",
            "        \"name\": \"example_assistant\",\n",
            "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
            "    },\n",
            "    {\n",
            "        \"role\": \"user\",\n",
            "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
            "    },\n",
            "]\n",
            "\n",
            "for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n",
            "    print(model)\n",
            "    # example token count from the function defined above\n",
            "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
            "    # example token count from the OpenAI API\n",
            "    response = openai.ChatCompletion.create(\n",
            "        model=model,\n",
            "        messages=example_messages,\n",
            "        temperature=0,\n",
            "        max_tokens=1  # we're only counting input tokens here, so let's not waste tokens on the output\n",
            "    )\n",
            "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
            "    print()\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.10"
      },
      "vscode": {
         "interpreter": {
            "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
         }
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}