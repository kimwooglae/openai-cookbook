{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 틱토큰으로 토큰을 계산하는 방법\n",
    "\n",
    "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md)은 OpenAI의 빠른 오픈소스 토큰 생성기입니다.\n",
    "\n",
    "토큰화기는 텍스트 문자열(예: `\"tiktoken is great!\"`)과 인코딩(예: `\"cl100k_base\"`)이 주어지면 텍스트 문자열을 토큰 목록(예: `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`)으로 분할할 수 있습니다.\n",
    "\n",
    "텍스트 문자열을 토큰으로 분할하는 것은 GPT 모델이 텍스트를 토큰 형태로 보기 때문에 유용합니다. 텍스트 문자열에 몇 개의 토큰이 있는지 알면 (1) 텍스트 모델이 처리하기에 문자열이 너무 긴지 여부와 (2) OpenAI API 호출 비용이 얼마인지 알 수 있습니다(사용량은 토큰별로 가격이 책정되므로). 모델마다 다른 인코딩을 사용합니다.\n",
    "\n",
    "---\n",
    "\n",
    "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md) is a fast open-source tokenizer by OpenAI.\n",
    "\n",
    "Given a text string (e.g., `\"tiktoken is great!\"`) and an encoding (e.g., `\"cl100k_base\"`), a tokenizer can split the text string into a list of tokens (e.g., `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`).\n",
    "\n",
    "Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token). Different models use different encodings.\n",
    "\n",
    "\n",
    "## 인코딩\n",
    "\n",
    "인코딩은 텍스트가 토큰으로 변환되는 방식을 지정합니다. 모델마다 다른 인코딩을 사용합니다.\n",
    "\n",
    "`tiktoken`은 OpenAI 모델에서 사용하는 세 가지 인코딩을 지원합니다:\n",
    "\n",
    "| 인코딩 이름 | OpenAI 모델 |\n",
    "|-------------------------|-----------------------------------------------------|\n",
    "| `cl100k_base`           | ChatGPT models, `text-embedding-ada-002`            |\n",
    "| `p50k_base`             | Code models, `text-davinci-002`, `text-davinci-003` |\n",
    "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
    "\n",
    "틱토큰의 모델 인코딩은 다음과 같이 `tiktoken.encoding_for_model()`을 사용하여 검색할 수 있습니다:\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "```\n",
    "\n",
    "`p50k_base`는 `r50k_base`와 상당 부분 겹치며, 코드가 아닌 애플리케이션의 경우 일반적으로 동일한 토큰을 제공합니다.\n",
    "\n",
    "---\n",
    "\n",
    "Encodings specify how text is converted into tokens. Different models use different encodings.\n",
    "\n",
    "`tiktoken` supports three encodings used by OpenAI models:\n",
    "\n",
    "| Encoding name           | OpenAI models                                       |\n",
    "|-------------------------|-----------------------------------------------------|\n",
    "| `cl100k_base`           | ChatGPT models, `text-embedding-ada-002`            |\n",
    "| `p50k_base`             | Code models, `text-davinci-002`, `text-davinci-003` |\n",
    "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
    "\n",
    "You can retrieve the encoding for a model using `tiktoken.encoding_for_model()` as follows:\n",
    "```python\n",
    "encoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n",
    "```\n",
    "\n",
    "`p50k_base` overlaps substantially with `r50k_base`, and for non-code applications, they will usually give the same tokens.\n",
    "\n",
    "\n",
    "## 언어별 토큰화 라이브러리\n",
    "\n",
    "`cl100k_base` 및 `p50k_base` 인코딩의 경우, 2023년 3월 현재 사용 가능한 토큰라이저는 `tiktoken`뿐입니다.\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
    "\n",
    "`r50k_base`(`gpt2`) 인코딩의 경우, 토큰화 도구는 여러 언어로 제공됩니다.\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (또는 [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
    "- 자바스크립트: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
    "- .NET/C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
    "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
    "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
    "\n",
    "(OpenAI는 타사 라이브러리를 보증하거나 보증하지 않습니다.)\n",
    "\n",
    "---\n",
    "\n",
    "For `cl100k_base` and `p50k_base` encodings, `tiktoken` is the only tokenizer available as of March 2023.\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
    "\n",
    "For `r50k_base` (`gpt2`) encodings, tokenizers are available in many languages.\n",
    "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (or alternatively [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
    "- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
    "- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
    "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
    "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
    "\n",
    "(OpenAI makes no endorsements or guarantees of third-party libraries.)\n",
    "\n",
    "\n",
    "## 문자열이 일반적으로 토큰화되는 방식\n",
    "\n",
    "영어에서 토큰의 길이는 일반적으로 한 문자에서 한 단어(예: `\"t\"` 또는 `\"great\"`)까지이지만, 일부 언어에서는 토큰이 한 문자보다 짧거나 한 단어보다 길 수 있습니다. 공백은 일반적으로 단어의 시작 부분으로 그룹화됩니다(예: `\"is \"` 또는 `\" \"`+`\"is\"` 대신 `\" is\"`). 문자열이 토큰화되는 방식은 [OpenAI Tokenizer](https://beta.openai.com/tokenizer)에서 빠르게 확인할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "In English, tokens commonly range in length from one character to one word (e.g., `\"t\"` or `\" great\"`), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., `\" is\"` instead of `\"is \"` or `\" \"`+`\"is\"`). You can quickly check how a string is tokenized at the [OpenAI Tokenizer](https://beta.openai.com/tokenizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install `tiktoken`\n",
    "\n",
    "Install `tiktoken` with `pip`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/site-packages (0.2.0)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.3.0-cp310-cp310-macosx_10_9_x86_64.whl (735 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.2/735.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2022.10.31)\n",
      "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/site-packages (from tiktoken) (2.28.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: lxml~=4.9 in /usr/local/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (4.9.2)\n",
      "Requirement already satisfied: filelock~=3.0 in /usr/local/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (3.9.0)\n",
      "Requirement already satisfied: pycryptodomex~=3.8 in /usr/local/lib/python3.10/site-packages (from blobfile>=2->tiktoken) (3.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken) (3.0.1)\n",
      "Installing collected packages: tiktoken\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.2.0\n",
      "    Uninstalling tiktoken-0.2.0:\n",
      "      Successfully uninstalled tiktoken-0.2.0\n",
      "Successfully installed tiktoken-0.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import `tiktoken`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tiktoken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 인코딩 로드하기\n",
    "\n",
    "`tiktoken.get_encoding()`을 사용하여 이름별로 인코딩을 로드합니다.\n",
    "\n",
    "처음 실행할 때 다운로드하려면 인터넷 연결이 필요합니다. 나중에 실행할 때는 인터넷 연결이 필요하지 않습니다.\n",
    "\n",
    "---\n",
    "\n",
    "Use `tiktoken.get_encoding()` to load an encoding by name.\n",
    "\n",
    "The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoding = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 모델 이름에 맞는 올바른 인코딩을 자동으로 로드하려면 `tiktoken.encoding_for_model()`을 사용하세요.\n",
    "\n",
    "---\n",
    "\n",
    "Use `tiktoken.encoding_for_model()` to automatically load the correct encoding for a given model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# encoding = tiktoken.encoding_for_model(\"text-davinci-003\")\n",
    "encoding = tiktoken.encoding_for_model(\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `encoding.encode()`로 텍스트를 토큰으로 변환하기\n",
    "\n",
    "`.encode()` 메서드는 텍스트 문자열을 토큰 정수의 목록으로 변환합니다.\n",
    "\n",
    "---\n",
    "\n",
    "The `.encode()` method converts a text string into a list of token integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[169,\n",
       " 233,\n",
       " 109,\n",
       " 169,\n",
       " 228,\n",
       " 58260,\n",
       " 223,\n",
       " 108,\n",
       " 34804,\n",
       " 10997,\n",
       " 249,\n",
       " 234,\n",
       " 16306,\n",
       " 255,\n",
       " 61938,\n",
       " 0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding.encode(\"tiktoken is great!\")\n",
    "encoding.encode(\"틱토큰은 훌륭합니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.encode()`가 반환한 목록의 길이를 계산하여 토큰 수를 계산합니다.\n",
    "\n",
    "---\n",
    "\n",
    "Count tokens by counting the length of the list returned by `.encode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n",
    "num_tokens_from_string(\"틱토큰은 훌륭합니다!\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `encoding.decode()`로 토큰을 텍스트로 변환하기\n",
    "\n",
    "`.decode()`는 토큰 정수 목록을 문자열로 변환합니다.\n",
    "\n",
    "---\n",
    "\n",
    "`.decode()` converts a list of token integers to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'틱토큰은 훌륭합니다!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encoding.decode([83, 1609, 5963, 374, 2294, 0])\n",
    "encoding.decode([169, 233, 109, 169, 228, 58260, 223, 108, 34804, 10997, 249, 234, 16306, 255, 61938, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경고: `.decode()`는 단일 토큰에 적용될 수 있지만, utf-8 경계에 있지 않은 토큰의 경우 손실이 발생할 수 있으니 주의하세요.\n",
    "\n",
    "단일 토큰의 경우 `.decode_single_token_bytes()`는 단일 정수 토큰을 해당 토큰이 나타내는 바이트 수로 안전하게 변환합니다.\n",
    "\n",
    "---\n",
    "\n",
    "Warning: although `.decode()` can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries.\n",
    "\n",
    "For single tokens, `.decode_single_token_bytes()` safely converts a single integer token to the bytes it represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'\\xed',\n",
       " b'\\x8b',\n",
       " b'\\xb1',\n",
       " b'\\xed',\n",
       " b'\\x86',\n",
       " b'\\xa0\\xed',\n",
       " b'\\x81',\n",
       " b'\\xb0',\n",
       " b'\\xec\\x9d\\x80',\n",
       " b' \\xed',\n",
       " b'\\x9b',\n",
       " b'\\x8c',\n",
       " b'\\xeb\\xa5',\n",
       " b'\\xad',\n",
       " b'\\xed\\x95\\xa9\\xeb\\x8b\\x88\\xeb\\x8b\\xa4',\n",
       " b'!']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]\n",
    "[encoding.decode_single_token_bytes(token) for token in [169, 233, 109, 169, 228, 58260, 223, 108, 34804, 10997, 249, 234, 16306, 255, 61938, 0]]\n",
    "# [encoding.decode([token]) for token in [169, 233, 109, 169, 228, 58260, 223, 108, 34804, 10997, 249, 234, 16306, 255, 61938, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(문자열 앞의 `b`는 문자열이 바이트 문자열임을 나타냅니다.)\n",
    "\n",
    "(The `b` in front of the strings indicates that the strings are byte strings.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 인코딩 비교하기\n",
    "\n",
    "인코딩에 따라 단어를 분할하고, 공백을 그룹화하고, 영어가 아닌 문자를 처리하는 방식이 다를 수 있습니다. 위의 방법을 사용하여 몇 가지 예제 문자열에서 서로 다른 인코딩을 비교할 수 있습니다.\n",
    "\n",
    "Different encodings can vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_encodings(example_string: str) -> None:\n",
    "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
    "    # print the example string\n",
    "    print(f'\\nExample string: \"{example_string}\"')\n",
    "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
    "    for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:\n",
    "        encoding = tiktoken.get_encoding(encoding_name)\n",
    "        token_integers = encoding.encode(example_string)\n",
    "        num_tokens = len(token_integers)\n",
    "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
    "        print()\n",
    "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
    "        print(f\"token integers: {token_integers}\")\n",
    "        print(f\"token bytes: {token_bytes}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"틱토큰은 훌륭합니다!\"\n",
      "\n",
      "gpt2: 25 tokens\n",
      "token integers: [169, 233, 109, 169, 228, 254, 169, 223, 108, 35975, 222, 220, 169, 249, 234, 167, 98, 255, 47991, 102, 46695, 230, 46695, 97, 0]\n",
      "token bytes: [b'\\xed', b'\\x8b', b'\\xb1', b'\\xed', b'\\x86', b'\\xa0', b'\\xed', b'\\x81', b'\\xb0', b'\\xec\\x9d', b'\\x80', b' ', b'\\xed', b'\\x9b', b'\\x8c', b'\\xeb', b'\\xa5', b'\\xad', b'\\xed\\x95', b'\\xa9', b'\\xeb\\x8b', b'\\x88', b'\\xeb\\x8b', b'\\xa4', b'!']\n",
      "\n",
      "p50k_base: 25 tokens\n",
      "token integers: [169, 233, 109, 169, 228, 254, 169, 223, 108, 35975, 222, 220, 169, 249, 234, 167, 98, 255, 47991, 102, 46695, 230, 46695, 97, 0]\n",
      "token bytes: [b'\\xed', b'\\x8b', b'\\xb1', b'\\xed', b'\\x86', b'\\xa0', b'\\xed', b'\\x81', b'\\xb0', b'\\xec\\x9d', b'\\x80', b' ', b'\\xed', b'\\x9b', b'\\x8c', b'\\xeb', b'\\xa5', b'\\xad', b'\\xed\\x95', b'\\xa9', b'\\xeb\\x8b', b'\\x88', b'\\xeb\\x8b', b'\\xa4', b'!']\n",
      "\n",
      "cl100k_base: 16 tokens\n",
      "token integers: [169, 233, 109, 169, 228, 58260, 223, 108, 34804, 10997, 249, 234, 16306, 255, 61938, 0]\n",
      "token bytes: [b'\\xed', b'\\x8b', b'\\xb1', b'\\xed', b'\\x86', b'\\xa0\\xed', b'\\x81', b'\\xb0', b'\\xec\\x9d\\x80', b' \\xed', b'\\x9b', b'\\x8c', b'\\xeb\\xa5', b'\\xad', b'\\xed\\x95\\xa9\\xeb\\x8b\\x88\\xeb\\x8b\\xa4', b'!']\n"
     ]
    }
   ],
   "source": [
    "# compare_encodings(\"antidisestablishmentarianism\")\n",
    "compare_encodings(\"틱토큰은 훌륭합니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"2 + 2 = 4\"\n",
      "\n",
      "gpt2: 5 tokens\n",
      "token integers: [17, 1343, 362, 796, 604]\n",
      "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
      "\n",
      "p50k_base: 5 tokens\n",
      "token integers: [17, 1343, 362, 796, 604]\n",
      "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
      "\n",
      "cl100k_base: 7 tokens\n",
      "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
      "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"2 + 2 = 4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: \"お誕生日おめでとう\"\n",
      "\n",
      "gpt2: 14 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "p50k_base: 14 tokens\n",
      "token integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\n",
      "token bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n",
      "\n",
      "cl100k_base: 9 tokens\n",
      "token integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\n",
      "token bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n"
     ]
    }
   ],
   "source": [
    "compare_encodings(\"お誕生日おめでとう\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 채팅 API 호출에 대한 토큰 카운팅\n",
    "\n",
    "`gpt-3.5-turbo`와 같은 ChatGPT 모델은 다른 모델과 같은 방식으로 토큰을 사용하지만, 메시지 기반 형식이기 때문에 대화에 사용되는 토큰의 수를 계산하기가 더 어렵습니다.\n",
    "\n",
    "아래는 `gpt-3.5-turbo-0301`에 전달된 메시지의 토큰을 계산하는 예제 함수입니다.\n",
    "\n",
    "메시지가 토큰으로 변환되는 정확한 방식은 모델마다 다를 수 있습니다. 따라서 향후 모델 버전이 출시되면 이 함수가 반환하는 답변은 근사치에 불과할 수 있습니다. [ChatML 문서](https://github.com/openai/openai-python/blob/main/chatml.md)에서 OpenAI API가 메시지를 토큰으로 변환하는 방법을 설명하고 있으며, 자체 함수를 작성하는 데 유용할 수 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "ChatGPT models like `gpt-3.5-turbo` use tokens in the same way as other models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n",
    "\n",
    "Below is an example function for counting tokens for messages passed to `gpt-3.5-turbo-0301`.\n",
    "\n",
    "The exact way that messages are converted into tokens may change from model to model. So when future model versions are released, the answers returned by this function may be only approximate. The [ChatML documentation](https://github.com/openai/openai-python/blob/main/chatml.md) explains how messages are converted into tokens by the OpenAI API, and may be useful for writing your own function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
    "        num_tokens = 0\n",
    "        for message in messages:\n",
    "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "            for key, value in message.items():\n",
    "                num_tokens += len(encoding.encode(value))\n",
    "                if key == \"name\":  # if there's a name, the role is omitted\n",
    "                    num_tokens += -1  # role is always required and always 1 token\n",
    "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "        return num_tokens\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\n",
    "See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
    "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
    "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
    "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
    "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
    "    {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 prompt tokens counted.\n"
     ]
    }
   ],
   "source": [
    "# example token count from the function defined above\n",
    "model = \"gpt-3.5-turbo-0301\"\n",
    "\n",
    "print(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 prompt tokens used.\n"
     ]
    }
   ],
   "source": [
    "# example token count from the OpenAI API\n",
    "import openai\n",
    "\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    ")\n",
    "\n",
    "print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
