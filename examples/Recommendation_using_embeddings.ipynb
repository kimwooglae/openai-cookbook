{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 임베딩 및 가장 가까운 이웃 검색을 사용한 추천\n",
    "\n",
    "추천은 웹 전반에 걸쳐 널리 사용되고 있습니다.\n",
    "\n",
    "- '그 상품을 구매하셨나요? 비슷한 상품도 사용해 보세요.\n",
    "- '그 책 재미있었나요? 비슷한 제목의 책을 읽어보세요.\n",
    "- '찾고 있던 도움말 페이지가 없나요? 비슷한 페이지를 찾아보세요.\n",
    "\n",
    "이 노트북은 임베딩을 사용해 추천할 유사한 항목을 찾는 방법을 보여줍니다. 특히, [AG의 뉴스 기사 말뭉치](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)를 데이터셋으로 사용합니다.\n",
    "\n",
    "이 모델은 '어떤 기사가 주어졌을 때, 이 기사와 가장 유사한 다른 기사는 무엇인가'라는 질문에 답합니다.\n",
    "\n",
    "----\n",
    "\n",
    "Recommendations are widespread across the web.\n",
    "\n",
    "- 'Bought that item? Try these similar items.'\n",
    "- 'Enjoy that book? Try these similar titles.'\n",
    "- 'Not the help page you were looking for? Try these similar pages.'\n",
    "\n",
    "This notebook demonstrates how to use embeddings to find similar items to recommend. In particular, we use [AG's corpus of news articles](http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html) as our dataset.\n",
    "\n",
    "Our model will answer the question: given an article, what other articles are most similar to it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 임포트\n",
    "\n",
    "먼저 나중에 필요한 패키지와 함수를 가져옵니다. 이 패키지들이 없다면 직접 설치해야 합니다. 터미널에서 `pip install {package_name}`을 실행하여 설치할 수 있습니다(예: `pip install pandas`).\n",
    "\n",
    "---\n",
    "\n",
    "First, let's import the packages and functions we'll need for later. If you don't have these, you'll need to install them. You can install them via your terminal by running `pip install {package_name}`, e.g. `pip install pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from openai.embeddings_utils import (\n",
    "    get_embedding,\n",
    "    distances_from_embeddings,\n",
    "    tsne_components_from_embeddings,\n",
    "    chart_from_components,\n",
    "    indices_of_nearest_neighbors_from_distances,\n",
    ")\n",
    "\n",
    "# constants\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 로드\n",
    "\n",
    "다음으로 AG 뉴스 데이터를 로드하여 어떤 모습인지 확인해 보겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "Next, let's load the AG news data and see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>label_int</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>World Briefings</td>\n",
       "      <td>BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime M...</td>\n",
       "      <td>1</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nvidia Puts a Firewall on a Motherboard (PC Wo...</td>\n",
       "      <td>PC World - Upcoming chip set will include buil...</td>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Olympic joy in Greek, Chinese press</td>\n",
       "      <td>Newspapers in Greece reflect a mixture of exhi...</td>\n",
       "      <td>2</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U2 Can iPod with Pictures</td>\n",
       "      <td>SAN JOSE, Calif. -- Apple Computer (Quote, Cha...</td>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Dream Factory</td>\n",
       "      <td>Any product, any shape, any size -- manufactur...</td>\n",
       "      <td>4</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                                    World Briefings   \n",
       "1  Nvidia Puts a Firewall on a Motherboard (PC Wo...   \n",
       "2                Olympic joy in Greek, Chinese press   \n",
       "3                          U2 Can iPod with Pictures   \n",
       "4                                  The Dream Factory   \n",
       "\n",
       "                                         description  label_int     label  \n",
       "0  BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime M...          1     World  \n",
       "1  PC World - Upcoming chip set will include buil...          4  Sci/Tech  \n",
       "2  Newspapers in Greece reflect a mixture of exhi...          2    Sports  \n",
       "3  SAN JOSE, Calif. -- Apple Computer (Quote, Cha...          4  Sci/Tech  \n",
       "4  Any product, any shape, any size -- manufactur...          4  Sci/Tech  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data (full dataset available at http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html)\n",
    "dataset_path = \"data/AG_news_samples.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# print dataframe\n",
    "n_examples = 5\n",
    "df.head(n_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "줄임표로 잘리지 않은 동일한 예시를 살펴보겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "Let's take a look at those same examples, but not truncated by ellipsis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Title: World Briefings\n",
      "Description: BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime Minister Tony Blair urged the international community to consider global warming a dire threat and agree on a plan of action to curb the  quot;alarming quot; growth of greenhouse gases.\n",
      "Label: World\n",
      "\n",
      "Title: Nvidia Puts a Firewall on a Motherboard (PC World)\n",
      "Description: PC World - Upcoming chip set will include built-in security features for your PC.\n",
      "Label: Sci/Tech\n",
      "\n",
      "Title: Olympic joy in Greek, Chinese press\n",
      "Description: Newspapers in Greece reflect a mixture of exhilaration that the Athens Olympics proved successful, and relief that they passed off without any major setback.\n",
      "Label: Sports\n",
      "\n",
      "Title: U2 Can iPod with Pictures\n",
      "Description: SAN JOSE, Calif. -- Apple Computer (Quote, Chart) unveiled a batch of new iPods, iTunes software and promos designed to keep it atop the heap of digital music players.\n",
      "Label: Sci/Tech\n",
      "\n",
      "Title: The Dream Factory\n",
      "Description: Any product, any shape, any size -- manufactured on your desktop! The future is the fabricator. By Bruce Sterling from Wired magazine.\n",
      "Label: Sci/Tech\n"
     ]
    }
   ],
   "source": [
    "# print the title, description, and label of each example\n",
    "for idx, row in df.head(n_examples).iterrows():\n",
    "    print(\"\")\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Description: {row['description']}\")\n",
    "    print(f\"Label: {row['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 임베딩을 저장하기 위한 캐시 구축\n",
    "\n",
    "이 글에 대한 임베딩을 받기 전에 생성한 임베딩을 저장할 캐시를 설정해 보겠습니다. 일반적으로 나중에 다시 사용할 수 있도록 임베딩을 저장하는 것이 좋습니다. 저장하지 않으면 임베딩을 다시 계산할 때마다 비용을 다시 지불해야 합니다.\n",
    "\n",
    "캐시는 `(text, model)`의 튜플을 플로트 목록인 임베딩에 매핑하는 딕셔너리입니다. 캐시는 파이썬 피클 파일로 저장됩니다.\n",
    "\n",
    "----------\n",
    "\n",
    "Before getting embeddings for these articles, let's set up a cache to save the embeddings we generate. In general, it's a good idea to save your embeddings so you can re-use them later. If you don't save them, you'll pay again each time you compute them again.\n",
    "\n",
    "The cache is a dictionary that maps tuples of `(text, model)` to an embedding, which is a list of floats. The cache is saved as a Python pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish a cache of embeddings to avoid recomputing\n",
    "# cache is a dict of tuples (text, model) -> embedding, saved as a pickle file\n",
    "\n",
    "# set path to embedding cache\n",
    "embedding_cache_path = \"data/recommendations_embeddings_cache.pkl\"\n",
    "\n",
    "# load the cache if it exists, and save a copy to disk\n",
    "try:\n",
    "    embedding_cache = pd.read_pickle(embedding_cache_path)\n",
    "except FileNotFoundError:\n",
    "    embedding_cache = {}\n",
    "with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "    pickle.dump(embedding_cache, embedding_cache_file)\n",
    "\n",
    "# define a function to retrieve embeddings from the cache if present, and otherwise request via the API\n",
    "def embedding_from_string(\n",
    "    string: str,\n",
    "    model: str = EMBEDDING_MODEL,\n",
    "    embedding_cache=embedding_cache\n",
    ") -> list:\n",
    "    \"\"\"Return embedding of given string, using a cache to avoid recomputing.\"\"\"\n",
    "    if (string, model) not in embedding_cache.keys():\n",
    "        embedding_cache[(string, model)] = get_embedding(string, model)\n",
    "        with open(embedding_cache_path, \"wb\") as embedding_cache_file:\n",
    "            pickle.dump(embedding_cache, embedding_cache_file)\n",
    "    return embedding_cache[(string, model)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "임베딩을 통해 작동하는지 확인해 보겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "Let's check that it works by getting an embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example string: BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime Minister Tony Blair urged the international community to consider global warming a dire threat and agree on a plan of action to curb the  quot;alarming quot; growth of greenhouse gases.\n",
      "\n",
      "Example embedding: [-0.01071077398955822, -0.022362446412444115, -0.00883542187511921, -0.0254171434789896, 0.031423427164554596, 0.010723662562668324, -0.016717055812478065, 0.004195375367999077, -0.008074969984591007, -0.02142154797911644]...\n"
     ]
    }
   ],
   "source": [
    "# as an example, take the first description from the dataset\n",
    "example_string = df[\"description\"].values[0]\n",
    "print(f\"\\nExample string: {example_string}\")\n",
    "\n",
    "# print the first 10 dimensions of the embedding\n",
    "example_embedding = embedding_from_string(example_string)\n",
    "print(f\"\\nExample embedding: {example_embedding[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 임베딩을 기반으로 유사한 문서 추천\n",
    "\n",
    "유사한 기사를 찾으려면 3단계 계획을 따르세요:\n",
    "1. 모든 문서 설명의 유사성 임베딩을 가져옵니다.\n",
    "2. 소스 제목과 다른 모든 기사 사이의 거리를 계산합니다.\n",
    "3. 소스 제목과 가장 가까운 다른 기사를 인쇄합니다.\n",
    "\n",
    "------\n",
    "\n",
    "To find similar articles, let's follow a three-step plan:\n",
    "1. Get the similarity embeddings of all the article descriptions\n",
    "2. Calculate the distance between a source title and all other articles\n",
    "3. Print out the other articles closest to the source title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_recommendations_from_strings(\n",
    "    strings: list[str],\n",
    "    index_of_source_string: int,\n",
    "    k_nearest_neighbors: int = 1,\n",
    "    model=EMBEDDING_MODEL,\n",
    ") -> list[int]:\n",
    "    \"\"\"Print out the k nearest neighbors of a given string.\"\"\"\n",
    "    # 모든 문자열에 대한 임베딩 가져오기\n",
    "    embeddings = [embedding_from_string(string, model=model) for string in strings]\n",
    "    # 소스 문자열 임베딩 가져오기\n",
    "    query_embedding = embeddings[index_of_source_string]\n",
    "    # 소스 임베딩과 다른 임베딩 사이의 거리 가져오기(embeddings_utils.py의 함수)\n",
    "    distances = distances_from_embeddings(query_embedding, embeddings, distance_metric=\"cosine\")\n",
    "    # 가장 가까운 이웃의 인덱스 가져오기(embeddings_utils.py의 함수)\n",
    "    indices_of_nearest_neighbors = indices_of_nearest_neighbors_from_distances(distances)\n",
    "\n",
    "    # print out source string\n",
    "    query_string = strings[index_of_source_string]\n",
    "    print(f\"Source string: {query_string}\")\n",
    "    # print out its k nearest neighbors\n",
    "    k_counter = 0\n",
    "    for i in indices_of_nearest_neighbors:\n",
    "        # 시작 문자열과 동일한 문자열은 건너뜁니다.\n",
    "        if query_string == strings[i]:\n",
    "            continue\n",
    "        # k개의 기사를 출력한 후 중지\n",
    "        if k_counter >= k_nearest_neighbors:\n",
    "            break\n",
    "        k_counter += 1\n",
    "\n",
    "        # print out the similar strings and their distances\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        --- Recommendation #{k_counter} (nearest neighbor {k_counter} of {k_nearest_neighbors}) ---\n",
    "        String: {strings[i]}\n",
    "        Distance: {distances[i]:0.3f}\"\"\"\n",
    "        )\n",
    "\n",
    "    return indices_of_nearest_neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 추천 예시\n",
    "\n",
    "토니 블레어에 관한 첫 번째 기사와 유사한 기사를 찾아보겠습니다.\n",
    "\n",
    "----\n",
    "\n",
    "Let's look for articles similar to first one, which was about Tony Blair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source string: BRITAIN: BLAIR WARNS OF CLIMATE THREAT Prime Minister Tony Blair urged the international community to consider global warming a dire threat and agree on a plan of action to curb the  quot;alarming quot; growth of greenhouse gases.\n",
      "\n",
      "        --- Recommendation #1 (nearest neighbor 1 of 5) ---\n",
      "        String: THE re-election of British Prime Minister Tony Blair would be seen as an endorsement of the military action in Iraq, Prime Minister John Howard said today.\n",
      "        Distance: 0.153\n",
      "\n",
      "        --- Recommendation #2 (nearest neighbor 2 of 5) ---\n",
      "        String: LONDON, England -- A US scientist is reported to have observed a surprising jump in the amount of carbon dioxide, the main greenhouse gas.\n",
      "        Distance: 0.160\n",
      "\n",
      "        --- Recommendation #3 (nearest neighbor 3 of 5) ---\n",
      "        String: The anguish of hostage Kenneth Bigley in Iraq hangs over Prime Minister Tony Blair today as he faces the twin test of a local election and a debate by his Labour Party about the divisive war.\n",
      "        Distance: 0.160\n",
      "\n",
      "        --- Recommendation #4 (nearest neighbor 4 of 5) ---\n",
      "        String: Israel is prepared to back a Middle East conference convened by Tony Blair early next year despite having expressed fears that the British plans were over-ambitious and designed \n",
      "        Distance: 0.171\n",
      "\n",
      "        --- Recommendation #5 (nearest neighbor 5 of 5) ---\n",
      "        String: AFP - A battle group of British troops rolled out of southern Iraq on a US-requested mission to deadlier areas near Baghdad, in a major political gamble for British Prime Minister Tony Blair.\n",
      "        Distance: 0.173\n"
     ]
    }
   ],
   "source": [
    "article_descriptions = df[\"description\"].tolist()\n",
    "\n",
    "tony_blair_articles = print_recommendations_from_strings(\n",
    "    strings=article_descriptions, # 기사 설명을 기반으로 유사성을 계산합니다.\n",
    "    index_of_source_string=0, # 토니 블레어에 대한 첫 번째 기사와 유사한 기사를 살펴봅시다.\n",
    "    k_nearest_neighbors=5, # 가장 유사한 기사 5개를 살펴봅시다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "꽤 괜찮네요! 5개의 추천 항목 중 4개는 토니 블레어를 명시적으로 언급하고 있으며, 다섯 번째는 토니 블레어와 자주 연관될 수 있는 주제인 기후 변화에 관한 런던의 기사입니다.\n",
    "\n",
    "---\n",
    "\n",
    "Pretty good! 4 of the 5 recommendations explicitly mention Tony Blair and the fifth is an article from London about climate change, topics that might be often associated with Tony Blair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보안이 강화된 NVIDIA의 새로운 칩셋에 대한 두 번째 예제 기사에서 추천 제품이 어떻게 작동하는지 확인해 보겠습니다.\n",
    "\n",
    "---\n",
    "\n",
    "Let's see how our recommender does on the second example article about NVIDIA's new chipset with more security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source string: PC World - Upcoming chip set will include built-in security features for your PC.\n",
      "\n",
      "        --- Recommendation #1 (nearest neighbor 1 of 5) ---\n",
      "        String: PC World - Updated antivirus software for businesses adds intrusion prevention features.\n",
      "        Distance: 0.112\n",
      "\n",
      "        --- Recommendation #2 (nearest neighbor 2 of 5) ---\n",
      "        String: PC World - The one-time World Class Product of the Year PDA gets a much-needed upgrade.\n",
      "        Distance: 0.145\n",
      "\n",
      "        --- Recommendation #3 (nearest neighbor 3 of 5) ---\n",
      "        String: PC World - Send your video throughout your house--wirelessly--with new gateways and media adapters.\n",
      "        Distance: 0.153\n",
      "\n",
      "        --- Recommendation #4 (nearest neighbor 4 of 5) ---\n",
      "        String: PC World - Symantec, McAfee hope raising virus-definition fees will move users to\\  suites.\n",
      "        Distance: 0.157\n",
      "\n",
      "        --- Recommendation #5 (nearest neighbor 5 of 5) ---\n",
      "        String: Gateway computers will be more widely available at Office Depot, in the PC maker #39;s latest move to broaden distribution at retail stores since acquiring rival eMachines this year. \n",
      "        Distance: 0.168\n"
     ]
    }
   ],
   "source": [
    "chipset_security_articles = print_recommendations_from_strings(\n",
    "    strings=article_descriptions, # 기사 설명에서 유사성을 기준으로 합니다.\n",
    "    index_of_source_string=1, # 더 안전한 칩셋에 대한 두 번째 기사와 유사한 기사를 살펴봅시다.\n",
    "    k_nearest_neighbors=5, # 가장 유사한 기사 5개를 살펴봅시다.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "인쇄된 거리를 보면 1위 추천이 다른 모든 추천보다 훨씬 가깝다는 것을 알 수 있습니다(0.11 대 0.14 이상). 그리고 1위 추천은 시작 기사와 매우 유사해 보이는데, 컴퓨터 보안 강화에 관한 PC World의 또 다른 기사입니다. 꽤 괜찮네요! \n",
    "\n",
    "------\n",
    "\n",
    "From the printed distances, you can see that the #1 recommendation is much closer than all the others (0.11 vs 0.14+). And the #1 recommendation looks very similar to the starting article - it's another article from PC World about increasing computer security. Pretty good! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: 보다 정교한 추천자에서 임베딩 사용\n",
    "\n",
    "추천 시스템을 보다 정교하게 구축하는 방법은 항목 인기도나 사용자 클릭 데이터와 같은 수십 또는 수백 개의 신호를 수신하는 머신 러닝 모델을 훈련하는 것입니다. 이 시스템에서도 임베딩은 추천 시스템에 매우 유용한 신호가 될 수 있으며, 특히 아직 사용자 데이터가 없는 '콜드 스타트' 중인 품목(예: 아직 클릭이 없는 카탈로그에 추가된 새 제품)의 경우 더욱 그렇습니다.\n",
    "\n",
    "---\n",
    "\n",
    "A more sophisticated way to build a recommender system is to train a machine learning model that takes in tens or hundreds of signals, such as item popularity or user click data. Even in this system, embeddings can be a very useful signal into the recommender, especially for items that are being 'cold started' with no user data yet (e.g., a brand new product added to the catalog without any clicks yet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: 임베딩을 사용하여 유사한 문서 시각화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 가까운 이웃 추천자가 어떤 일을 하고 있는지 이해하기 위해 기사 임베딩을 시각화해 보겠습니다. 각 임베딩 벡터의 2048개 차원을 플로팅할 수는 없지만, [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) 또는 [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis)와 같은 기술을 사용하여 임베딩을 2차원 또는 3차원으로 압축하여 차트로 나타낼 수 있습니다.\n",
    "\n",
    "가장 가까운 이웃을 시각화하기 전에 t-SNE를 사용하여 모든 기사 설명을 시각화해 보겠습니다. t-SNE는 결정론적이지 않으므로 실행할 때마다 결과가 달라질 수 있다는 점에 유의하세요.\n",
    "\n",
    "-------\n",
    "\n",
    "To get a sense of what our nearest neighbor recommender is doing, let's visualize the article embeddings. Although we can't plot the 2048 dimensions of each embedding vector, we can use techniques like [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) or [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) to compress the embeddings down into 2 or 3 dimensions, which we can chart.\n",
    "\n",
    "Before visualizing the nearest neighbors, let's visualize all of the article descriptions using t-SNE. Note that t-SNE is not deterministic, meaning that results may vary from run to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 문서 설명에 대한 임베딩 가져오기\n",
    "embeddings = [embedding_from_string(string) for string in article_descriptions]\n",
    "# t-SNE를 사용하여 2048차원 임베딩을 2차원으로 압축합니다.\n",
    "tsne_components = tsne_components_from_embeddings(embeddings)\n",
    "# 차트에 색을 입히기 위한 기사 레이블 가져오기\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "chart_from_components(\n",
    "    components=tsne_components,\n",
    "    labels=labels,\n",
    "    strings=article_descriptions,\n",
    "    width=600,\n",
    "    height=500,\n",
    "    title=\"t-SNE components of article descriptions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 차트에서 볼 수 있듯이 고도로 압축된 임베딩도 카테고리별로 문서 설명을 클러스터링하는 데 효과적입니다. 이 클러스터링은 레이블 자체에 대한 지식 없이도 수행된다는 점을 강조할 가치가 있습니다!\n",
    "\n",
    "또한 가장 심각한 이상값을 자세히 살펴보면 잘못된 임베딩보다는 잘못된 라벨링이 원인인 경우가 많습니다. 예를 들어, 녹색 스포츠 클러스터에 있는 파란색 월드 포인트의 대부분은 스포츠 스토리로 보입니다.\n",
    "\n",
    "---------\n",
    "\n",
    "As you can see in the chart above, even the highly compressed embeddings do a good job of clustering article descriptions by category. And it's worth emphasizing: this clustering is done with no knowledge of the labels themselves!\n",
    "\n",
    "Also, if you look closely at the most egregious outliers, they are often due to mislabeling rather than poor embedding. For example, the majority of the blue World points in the green Sports cluster appear to be Sports stories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로, 소스 문서인지, 가장 가까운 이웃 문서인지 또는 기타 문서인지에 따라 포인트의 색을 다시 지정해 보겠습니다.\n",
    "\n",
    "-----\n",
    "\n",
    "Next, let's recolor the points by whether they are a source article, its nearest neighbors, or other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추천 글에 대한 레이블 만들기\n",
    "def nearest_neighbor_labels(\n",
    "    list_of_indices: list[int],\n",
    "    k_nearest_neighbors: int = 5\n",
    ") -> list[str]:\n",
    "    \"\"\"Return a list of labels to color the k nearest neighbors.\"\"\"\n",
    "    labels = [\"Other\" for _ in list_of_indices]\n",
    "    source_index = list_of_indices[0]\n",
    "    labels[source_index] = \"Source\"\n",
    "    for i in range(k_nearest_neighbors):\n",
    "        nearest_neighbor_index = list_of_indices[i + 1]\n",
    "        labels[nearest_neighbor_index] = f\"Nearest neighbor (top {k_nearest_neighbors})\"\n",
    "    return labels\n",
    "\n",
    "\n",
    "tony_blair_labels = nearest_neighbor_labels(tony_blair_articles, k_nearest_neighbors=5)\n",
    "chipset_security_labels = nearest_neighbor_labels(chipset_security_articles, k_nearest_neighbors=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토니 블레어 기사의 가장 가까운 이웃을 보여주는 2D 차트\n",
    "chart_from_components(\n",
    "    components=tsne_components,\n",
    "    labels=tony_blair_labels,\n",
    "    strings=article_descriptions,\n",
    "    width=600,\n",
    "    height=500,\n",
    "    title=\"Nearest neighbors of the Tony Blair article\",\n",
    "    category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 2D 차트를 보면 토니 블레어에 대한 기사가 세계 뉴스 클러스터 내에서 어느 정도 서로 가깝다는 것을 알 수 있습니다. 흥미롭게도 가장 가까운 이웃 5개(빨간색)는 고차원 공간에서는 가장 가깝지만, 이 압축된 2D 공간에서는 가장 가까운 지점이 아닙니다. 임베딩을 2차원으로 압축하면 많은 정보가 손실되며, 2D 공간에서 가장 가까운 이웃은 전체 임베딩 공간의 이웃만큼 관련성이 높지 않은 것으로 보입니다.\n",
    "\n",
    "----\n",
    "\n",
    "Looking at the 2D chart above, we can see that the articles about Tony Blair are somewhat close together inside of the World news cluster. Interestingly, although the 5 nearest neighbors (red) were closest in high dimensional space, they are not the closest points in this compressed 2D space. Compressing the embeddings down to 2 dimensions discards much of their information, and the nearest neighbors in the 2D space don't seem to be as relevant as those in the full embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 칩셋 보안 문서의 가장 가까운 이웃을 보여주는 2D 차트\n",
    "chart_from_components(\n",
    "    components=tsne_components,\n",
    "    labels=chipset_security_labels,\n",
    "    strings=article_descriptions,\n",
    "    width=600,\n",
    "    height=500,\n",
    "    title=\"Nearest neighbors of the chipset security article\",\n",
    "    category_orders={\"label\": [\"Other\", \"Nearest neighbor (top 5)\", \"Source\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "칩셋 보안 예제의 경우, 전체 임베딩 공간에서 가장 가까운 4개의 이웃이 이 압축된 2D 시각화에서 가장 가까운 이웃으로 남아 있습니다. 다섯 번째는 전체 임베딩 공간에서는 더 가깝지만 더 먼 것으로 표시됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "For the chipset security example, the 4 closest nearest neighbors in the full embedding space remain nearest neighbors in this compressed 2D visualization. The fifth is displayed as more distant, despite being closer in the full embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원한다면 `chart_from_components_3D` 함수를 사용하여 임베딩의 대화형 3D 플롯을 만들 수도 있습니다. (이렇게 하려면 `n_components=3`으로 t-SNE 컴포넌트를 다시 계산해야 합니다.)\n",
    "\n",
    "---\n",
    "\n",
    "Should you want to, you can also make an interactive 3D plot of the embeddings with the function `chart_from_components_3D`. (Doing so will require recomputing the t-SNE components with `n_components=3`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
