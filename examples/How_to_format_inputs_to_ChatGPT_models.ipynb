{
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# ChatGPT 모델에 입력 포맷 지정하는 방법\n",
      "\n",
      "ChatGPT는 OpenAI의 가장 진보된 모델인 `gpt-3.5-turbo`로 구동됩니다.\n",
      "\n",
      "OpenAI API를 사용하여 `gpt-3.5-turbo`로 자신만의 애플리케이션을 구축할 수 있습니다.\n",
      "\n",
      "채팅 모델은 일련의 메시지를 입력으로 받아 AI가 작성한 메시지를 출력으로 반환합니다.\n",
      "\n",
      "이 가이드에서는 몇 가지 API 호출 예제를 통해 채팅 형식을 설명합니다.\n",
      "\n",
      "----\n",
      "\n",
      "ChatGPT is powered by `gpt-3.5-turbo` and `gpt-4`, OpenAI's most advanced models.\n",
      "\n",
      "You can build your own applications with `gpt-3.5-turbo` or `gpt-4` using the OpenAI API.\n",
      "\n",
      "Chat models take a series of messages as input, and return an AI-written message as output.\n",
      "\n",
      "This guide illustrates the chat format with a few example API calls."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. openai 라이브러리 가져오기"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "Requirement already satisfied: openai in /usr/local/lib/python3.10/site-packages (0.27.0)\n",
        "Collecting openai\n",
        "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
        "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
        "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/site-packages (from openai) (3.8.4)\n",
        "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/site-packages (from openai) (2.28.2)\n",
        "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from openai) (4.64.1)\n",
        "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.0.1)\n",
        "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
        "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
        "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.20->openai) (1.26.14)\n",
        "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
        "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
        "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (1.8.2)\n",
        "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (22.2.0)\n",
        "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
        "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
        "Installing collected packages: openai\n",
        "  Attempting uninstall: openai\n",
        "    Found existing installation: openai 0.27.0\n",
        "    Uninstalling openai-0.27.0:\n",
        "      Successfully uninstalled openai-0.27.0\n",
        "Successfully installed openai-0.27.2\n",
        "\n",
        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n",
        "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n",
        "Note: you may need to restart the kernel to use updated packages.\n"
       ]
      }
     ],
     "source": [
      "# if needed, install and/or upgrade to the latest version of the OpenAI Python library\n",
      "%pip install --upgrade openai\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "outputs": [],
     "source": [
      "# import the OpenAI Python library for calling the OpenAI API\n",
      "import openai\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 2. 채팅 API 호출 예시\n",
      "\n",
      "채팅 API 호출에는 두 가지 필수 입력이 있습니다:\n",
      "- `model`: 사용하려는 모델의 이름(예: `gpt-3.5-turbo`)\n",
      "- `message`: 메시지 객체의 목록으로, 각 객체에는 최소 두 개의 필드가 있습니다:\n",
      "    - `role`: 메신저의 역할 (`system`, `user`, 또는 |`assistant` 중 하나)\n",
      "    - `content`: 메시지의 내용(예: `아름다운 시를 써줘`)\n",
      "\n",
      "일반적으로 대화는 시스템 메시지로 시작한 다음 사용자 메시지와 어시스턴트 메시지가 번갈아 가며 나오지만, 반드시 이 형식을 따를 필요는 없습니다.\n",
      "\n",
      "채팅 API 호출 예시를 통해 채팅 형식이 실제로 어떻게 작동하는지 살펴봅시다.\n",
      "\n",
      "----\n",
      "\n",
      "A chat API call has two required inputs:\n",
      "- `model`: the name of the model you want to use (e.g., `gpt-3.5-turbo`, `gpt-4`, `gpt-4-0314`)\n",
      "- `messages`: a list of message objects, where each object has two required fields:\n",
      "    - `role`: the role of the messenger (either `system`, `user`, or `assistant`)\n",
      "    - `content`: the content of the message (e.g., `Write me a beautiful poem`)\n",
      "\n",
      "Messages can also contain an optional `name` field, which give the messenger a name. E.g., `example-user`, `Alice`, `BlackbeardBot`. Names may not contain spaces.\n",
      "\n",
      "Typically, a conversation will start with a system message that tells the assistant how to behave, followed by alternating user and assistant messages, but you are not required to follow this format.\n",
      "\n",
      "Let's look at an example chat API calls to see how the chat format works in practice."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "outputs": [
      {
       "data": {
        "text/plain": [
         "<OpenAIObject chat.completion id=chatcmpl-6tjfZbwIJWCogqfwkYQLzNpY9esEc at 0x114813470> JSON: {\n",
         "  \"choices\": [\n",
         "    {\n",
         "      \"finish_reason\": \"stop\",\n",
         "      \"index\": 0,\n",
         "      \"message\": {\n",
         "        \"content\": \"\\uc88b\\uc544\\uc694, \\uc624\\ub80c\\uc9c0! \\uc5b4\\ub5a4 \\ub3c4\\uc6c0\\uc774 \\ud544\\uc694\\ud558\\uc138\\uc694?\",\n",
         "        \"role\": \"assistant\"\n",
         "      }\n",
         "    }\n",
         "  ],\n",
         "  \"created\": 1678741613,\n",
         "  \"id\": \"chatcmpl-6tjfZbwIJWCogqfwkYQLzNpY9esEc\",\n",
         "  \"model\": \"gpt-3.5-turbo-0301\",\n",
         "  \"object\": \"chat.completion\",\n",
         "  \"usage\": {\n",
         "    \"completion_tokens\": 27,\n",
         "    \"prompt_tokens\": 53,\n",
         "    \"total_tokens\": 80\n",
         "  }\n",
         "}"
        ]
       },
       "execution_count": 2,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "source": [
      "# Example OpenAI Python library request\n",
      "MODEL = \"gpt-3.5-turbo\"\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=MODEL,\n",
      "    messages=[\n",
      "        {\"role\": \"system\", \"content\": \"당신은 유용한 조수입니다.\"},\n",
      "        {\"role\": \"user\", \"content\": \"똑똑.\"},\n",
      "        {\"role\": \"assistant\", \"content\": \"누구신가요?\"},\n",
      "        {\"role\": \"user\", \"content\": \"오렌지.\"},\n",
      "        # {\"role\": \"user\", \"content\": \"Knock knock.\"},\n",
      "        # {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
      "        # {\"role\": \"user\", \"content\": \"Orange.\"},\n",
      "    ],\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "response"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "보시다시피 응답 객체에는 몇 가지 필드가 있습니다:\n",
      "- `id`: 요청의 ID\n",
      "- `object`: 반환된 객체의 유형(예: `chat.completion`)\n",
      "- `created`: 요청의 타임스탬프\n",
      "- `model`: 응답을 생성하는 데 사용된 모델의 전체 이름\n",
      "- `usage`: 응답을 생성하는 데 사용된 토큰 수, 프롬프트, 완료 및 총계를 계산합니다.\n",
      "- `choices`: 완료 객체 목록(`n`을 1보다 크게 설정하지 않는 한 하나만)\n",
      "    - `message`: 모델에 의해 생성된 메시지 객체로, `role` 및 `message`이 포함됩니다.\n",
      "    - `finish_reason`: 모델이 텍스트 생성을 중지한 이유(`stop`, 또는 `max_tokens` 제한에 도달한 경우 `length`)\n",
      "    - `index`: 선택 목록에서 완료된 항목의 색인\n",
      "\n",
      "\n",
      "--------\n",
      "\n",
      "As you can see, the response object has a few fields:\n",
      "- `id`: the ID of the request\n",
      "- `object`: the type of object returned (e.g., `chat.completion`)\n",
      "- `created`: the timestamp of the request\n",
      "- `model`: the full name of the model used to generate the response\n",
      "- `usage`: the number of tokens used to generate the replies, counting prompt, completion, and total\n",
      "- `choices`: a list of completion objects (only one, unless you set `n` greater than 1)\n",
      "    - `message`: the message object generated by the model, with `role` and `content`\n",
      "    - `finish_reason`: the reason the model stopped generating text (either `stop`, or `length` if `max_tokens` limit was reached)\n",
      "    - `index`: the index of the completion in the list of choices"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "회신만 추출합니다:\n",
      "\n",
      "----\n",
      "\n",
      "Extract just the reply with:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "outputs": [
      {
       "data": {
        "text/plain": [
         "'좋아요, 오렌지! 어떤 도움이 필요하세요?'"
        ]
       },
       "execution_count": 3,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "source": [
      "response['choices'][0]['message']['content']"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "대화 기반이 아닌 작업도 첫 번째 사용자 메시지에 지침을 배치하여 채팅 형식에 맞출 수 있습니다.\n",
      "\n",
      "예를 들어 모델에게 해적 검은수염의 스타일로 비동기 프로그래밍을 설명해 달라고 요청하려면 다음과 같이 대화를 구성할 수 있습니다:\n",
      "\n",
      "---\n",
      "\n",
      "Even non-conversation-based tasks can fit into the chat format, by placing the instruction in the first user message.\n",
      "\n",
      "For example, to ask the model to explain asynchronous programming in the style of the pirate Blackbeard, we can structure conversation as follows:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "비동기 프로그래밍은 코드 실행 순서가 보장되지 않는 프로그래밍 방식입니다. 이는 일반적으로 네트워크 요청, 파일 입출력 등과 같이 시간이 오래 걸리는 작업을 수행할 때 사용됩니다. 이러한 작업은 일반적으로 동기적으로 수행되면 전체 애플리케이션이 멈추게 되므로, 비동기적으로 처리하여 애플리케이션이 멈추지 않도록 합니다.\n",
        "\n",
        "JavaScript에서는 비동기 프로그래밍을 위해 Promise, async/await 등의 기능을 제공합니다. Promise는 비동기 작업의 결과를 처리하기 위한 객체이며, async/await는 Promise를 더 쉽게 사용할 수 있도록 해주는 문법입니다.\n",
        "\n",
        "아래는 async/await를 이용한 JavaScript 코드 예제입니다.\n",
        "\n",
        "```javascript\n",
        "async function getData() {\n",
        "  try {\n",
        "    const response = await fetch('https://jsonplaceholder.typicode.com/todos/1');\n",
        "    const data = await response.json();\n",
        "    console.log(data);\n",
        "  } catch (error) {\n",
        "    console.error(error);\n",
        "  }\n",
        "}\n",
        "\n",
        "getData();\n",
        "```\n",
        "\n",
        "위 코드에서는 `async` 함수를 정의하고, `await` 키워드를 사용하여 비동기 작업을 처리합니다. `fetch` 함수를 호출하여 네트워크 요청을 보내고, `response.json()` 메서드를 호출하여 응답 데이터를 JSON 형식으로 파싱합니다. 이후 파싱된 데이터를 출력합니다.\n",
        "\n",
        "`async/await`를 사용하면 코드가 더 직관적이고 가독성이 좋아집니다. 또한 `try/catch` 구문을 사용하여 예외 처리를 할 수 있습니다.\n"
       ]
      }
     ],
     "source": [
      "# example with a system message\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=MODEL,\n",
      "    messages=[\n",
      "        {\"role\": \"system\", \"content\": \"당신은 유용한 조수입니다.\"},\n",
      "        # # {\"role\": \"user\", \"content\": \"초등학생도 이해할 수 있도록 비동기 프로그래밍 설명하기.\"},\n",
      "        # # {\"role\": \"user\", \"content\": \"해적 검은수염 스타일로 비동기 프로그래밍 설명하기.\"},\n",
      "        {\"role\": \"user\", \"content\": \"비동기 프로그래밍 설명하기.\\nasync, await를 이용한 JavaScript 코드 예제와 함께 설명해주세요.\"},\n",
      "\n",
      "        # 기존 예제 코드\n",
      "        # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
      "        # {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
      "    ],\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "print(response['choices'][0]['message']['content'])\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "\n",
        "\n",
        "비동기 프로그래밍은 코드 실행 순서와 관계없이 작업이 완료될 때까지 기다리지 않고 다음 코드를 실행하는 방식입니다. 이를 통해 더 효율적인 코드 실행이 가능해집니다.\n",
        "\n",
        "JavaScript에서는 비동기 프로그래밍을 위해 콜백 함수, Promise, async/await 등의 방법을 제공합니다. async/await는 Promise를 기반으로 한 문법적 설탕으로, 코드를 더 직관적으로 작성할 수 있게 해줍니다.\n",
        "\n",
        "아래는 async/await를 이용한 예제 코드입니다.\n",
        "\n",
        "```javascript\n",
        "async function getData() {\n",
        "  try {\n",
        "    const response = await fetch('https://jsonplaceholder.typicode.com/todos/1');\n",
        "    const data = await response.json();\n",
        "    console.log(data);\n",
        "  } catch (error) {\n",
        "    console.log(error);\n",
        "  }\n",
        "}\n",
        "\n",
        "getData();\n",
        "```\n",
        "\n",
        "위 코드에서는 async 함수인 getData()를 정의하고, 내부에서 fetch() 함수를 호출하여 비동기적으로 데이터를 가져옵니다. 이때 await 키워드를 사용하여 fetch() 함수가 완료될 때까지 기다린 후, response 변수에 결과를 할당합니다.\n",
        "\n",
        "이후 response.json() 함수를 호출하여 JSON 데이터를 파싱하고, 다시 await 키워드를 사용하여 파싱 결과를 data 변수에 할당합니다. 마지막으로 console.log() 함수를 호출하여 데이터를 출력합니다.\n",
        "\n",
        "만약 에러가 발생하면 catch 블록에서 에러를 처리합니다.\n",
        "\n",
        "위 코드에서는 async/await를 사용하여 비동기적으로 데이터를 가져오는 과정을 간결하게 표현할 수 있습니다.\n"
       ]
      }
     ],
     "source": [
      "# example without a system message\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=MODEL,\n",
      "    messages=[\n",
      "        {\"role\": \"user\", \"content\": \"비동기 프로그래밍 설명하기.\\nasync, await를 이용한 JavaScript 코드 예제와 함께 설명해주세요.\"},\n",
      "\n",
      "        # 기존 예제 코드\n",
      "        # {\"role\": \"user\", \"content\": \"Explain asynchronous programming in the style of the pirate Blackbeard.\"},\n",
      "    ],\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "print(response['choices'][0]['message']['content'])\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. gpt-3.5-turbo-0301 지시를 위한 팁\n",
      "\n",
      "모델 지시를 위한 모범 사례는 모델 버전에 따라 변경될 수 있습니다. 다음 조언은 `gpt-3.5-turbo-0301`에 적용되며 향후 모델에는 적용되지 않을 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "Best practices for instructing models may change from model version to model version. The advice that follows applies to `gpt-3.5-turbo-0301` and may not apply to future models."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 시스템 메시지\n",
      "\n",
      "시스템 메시지는 어시스턴트에게 다양한 성격이나 행동을 부여하는 데 사용할 수 있습니다.\n",
      "\n",
      "그러나 모델은 일반적으로 시스템 메시지에 많은 주의를 기울이지 않으므로 중요한 지침은 사용자 메시지에 배치하는 것이 좋습니다.\n",
      "\n",
      "---\n",
      "\n",
      "The system message can be used to prime the assistant with different personalities or behaviors.\n",
      "\n",
      "Be aware that `gpt-3.5-turbo-0301` does not generally pay as much attention to the system message as `gpt-4-0314`. Therefore, for `gpt-3.5-turbo-0301`, we recommend placing important instructions in the user message instead. Some developers have found success in continually moving the system message near the end of the conversation to keep the model's attention from drifting away as conversations get longer."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "분수는 하나의 수를 다른 수로 나눈 것을 나타내는 표현입니다. 분수는 분자와 분모로 이루어져 있습니다. 분자는 분수의 윗부분에 위치하며, 분모는 분수의 아랫부분에 위치합니다.\n",
        "\n",
        "분수의 원리는 다음과 같습니다. 두 수를 나누면 분자와 분모가 생기는데, 이때 분모는 분수의 값에 영향을 미칩니다. 분모가 작을수록 분수의 값은 커지고, 분모가 클수록 분수의 값은 작아집니다. 예를 들어, 1/2와 1/4를 비교하면, 1/2가 더 큰 값입니다. 이는 분모가 작기 때문입니다.\n",
        "\n",
        "또한, 분수는 소수와 비교하여 더 정확한 값을 나타낼 수 있습니다. 예를 들어, 1/3은 소수로 나타내면 0.3333...으로 무한소수가 되지만, 분수로 나타내면 정확한 값으로 나타낼 수 있습니다.\n",
        "\n",
        "이러한 분수의 원리를 이해하면, 분수를 더 쉽게 다룰 수 있고, 수학적 문제를 해결하는 데 도움이 됩니다.\n"
       ]
      }
     ],
     "source": [
      "# An example of a system message that primes the assistant to explain concepts in great depth\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=MODEL,\n",
      "    messages=[\n",
      "        {\"role\": \"system\", \"content\": \"친절하고 도움이 되는 조교입니다. 간단한 용어를 사용하여 개념을 심도 있게 설명하고 사람들이 학습하는 데 도움이 되는 예제를 제공합니다. 각 설명이 끝날 때마다 이해 여부를 확인하기 위해 질문을 합니다.\"},\n",
      "        {\"role\": \"user\", \"content\": \"분수의 원리를 설명해 주시겠어요?\"},\n",
      "\n",
      "        # 기존 예제 코드\n",
      "        # {\"role\": \"system\", \"content\": \"You are a friendly and helpful teaching assistant. You explain concepts in great depth using simple terms, and you give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"},\n",
      "        # {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
      "    ],\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "분수는 하나의 수를 다른 수로 나눈 것을 나타내는 표현입니다. 분수는 분자와 분모로 이루어져 있으며, 분자는 분수의 값에 기여하는 수이고, 분모는 분수의 값의 크기와 부호를 결정하는 수입니다.\n",
        "\n",
        "예를 들어, 2/3은 2를 3으로 나눈 분수입니다. 이 경우, 분자는 2이고 분모는 3입니다. 이 분수는 2를 3등분한 크기를 나타내며, 분모가 작을수록 분수의 값은 커지고, 분모가 클수록 분수의 값은 작아집니다.\n",
        "\n",
        "또한, 분모가 0이 되는 분수는 정의되지 않습니다. 이는 분모가 0이 되면 분수의 값이 무한대나 음의 무한대로 발산하기 때문입니다. 따라서, 분모가 0이 되는 분수는 존재하지 않습니다.\n"
       ]
      }
     ],
     "source": [
      "# An example of a system message that primes the assistant to give brief, to-the-point answers\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=MODEL,\n",
      "    messages=[\n",
      "        {\"role\": \"system\", \"content\": \"귀하는 간결한 어시스턴트입니다. 자세한 설명 없이 간결하고 핵심적인 답변으로 답장합니다.\"},\n",
      "        {\"role\": \"user\", \"content\": \"분수의 원리를 설명해 주시겠어요?\"},\n",
      "\n",
      "        # 기존 예제 코드\n",
      "        # {\"role\": \"system\", \"content\": \"You are a laconic assistant. You reply with brief, to-the-point answers with no elaboration.\"},\n",
      "        # {\"role\": \"user\", \"content\": \"Can you explain how fractions work?\"},\n",
      "    ],\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Few-shot 프롬프트\n",
      "\n",
      "어떤 경우에는 모델에게 원하는 것을 말하기보다 모델에게 원하는 것을 보여주는 것이 더 쉽습니다.\n",
      "\n",
      "모델에게 원하는 것을 보여주는 한 가지 방법은 가짜 예제 메시지를 사용하는 것입니다.\n",
      "\n",
      "예를 들어\n",
      "\n",
      "-------------\n",
      "\n",
      "In some cases, it's easier to show the model what you want rather than tell the model what you want.\n",
      "\n",
      "One way to show the model what you want is with faked example messages.\n",
      "\n",
      "For example:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 11,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "이미 늦은 시점에서 전략을 바꾸는 것은 고객의 요구사항을 충족시키는 데 시간이 부족하다는 것을 의미합니다.\n"
       ]
      }
     ],
     "source": [
      "# An example of a faked few-shot conversation to prime the model into translating business jargon to simpler speech\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=MODEL,\n",
      "    messages=[\n",
      "        {\"role\": \"system\", \"content\": \"귀하는 유용한 패턴을 따르는 어시스턴트입니다.\"},\n",
      "        {\"role\": \"user\", \"content\": \"다음 기업 전문 용어를 일반 언어로 번역하는 데 도움을 주세요.\"},\n",
      "        {\"role\": \"assistant\", \"content\": \"네, 기꺼이 도와드리겠습니다!\"},\n",
      "        {\"role\": \"user\", \"content\": \"새로운 시너지가 매출 성장에 도움이 될 것입니다.\"},\n",
      "        {\"role\": \"assistant\", \"content\": \"함께 잘 작동하면 수익이 증가합니다.\"},\n",
      "        {\"role\": \"user\", \"content\": \"더 많은 대역폭이 확보되면 다시 돌아와서 활용도를 높일 수 있는 기회에 대해 논의해 봅시다.\"},\n",
      "        {\"role\": \"assistant\", \"content\": \"더 잘할 수 있는 방법에 대해 나중에 덜 바쁠 때 다시 이야기합시다.\"},\n",
      "        {\"role\": \"user\", \"content\": \"이 늦은 피벗은 고객 결과물을 위해 바다를 끓일 시간이 없다는 것을 의미합니다.\"},\n",
      "               \n",
      "        \n",
      "        # 기존 예제 코드\n",
      "#         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant.\"},\n",
      "#         {\"role\": \"user\", \"content\": \"Help me translate the following corporate jargon into plain English.\"},\n",
      "#         {\"role\": \"assistant\", \"content\": \"Sure, I'd be happy to!\"},\n",
      "#         {\"role\": \"user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
      "#         {\"role\": \"assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
      "#         {\"role\": \"user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
      "#         {\"role\": \"assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
      "#         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
      "    ],\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "예제 메시지가 실제 대화의 일부가 아니며 모델에서 다시 참조해서는 안 된다는 점을 명확히 하기 위해, 대신 `system` 메시지의 `name` 필드를 `example_user` 및 `example_assistant`로 설정할 수 있습니다.\n",
      "\n",
      "위의 몇 가지 예시를 변형하면 다음과 같이 작성할 수 있습니다:\n",
      "\n",
      "-----------\n",
      "\n",
      "To help clarify that the example messages are not part of a real conversation, and shouldn't be referred back to by the model, you can instead set the `name` field of `system` messages to `example_user` and `example_assistant`.\n",
      "\n",
      "Transforming the few-shot example above, we could write:"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 12,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "이미 늦은 시점에서 전략을 바꾸는 것은 고객의 요구사항을 충족시키기 위한 시간이 부족하다는 것을 의미합니다.\n"
       ]
      }
     ],
     "source": [
      "# The business jargon translation example, but with example names for the example messages\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=MODEL,\n",
      "    messages=[\n",
      "        {\"role\": \"system\", \"content\": \"귀하는 기업 전문 용어를 일반 영어로 번역하는 패턴을 따르는 유용한 어시스턴트입니다.\"},\n",
      "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"새로운 시너지 효과로 매출 성장에 도움이 될 것입니다.\"},\n",
      "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"함께 잘 작동하면 수익이 증가합니다.\"},\n",
      "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"더 많은 대역폭이 확보되면 다시 돌아와서 활용도를 높일 수 있는 기회에 대해 논의해 봅시다.\"},\n",
      "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"더 잘할 수 있는 방법에 대해 나중에 덜 바쁠 때 이야기합시다.\"},\n",
      "        {\"role\": \"user\", \"content\": \"이 늦은 피벗은 고객 결과물을 위해 바다를 끓일 시간이 없다는 것을 의미합니다.\"},\n",
      "        \n",
      "#         기존 예제 코드\n",
      "#         {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
      "#         {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
      "#         {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
      "#         {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
      "#         {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
      "#         {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
      "    ],\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "print(response[\"choices\"][0][\"message\"][\"content\"])\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "대화에 대한 엔지니어링 시도가 처음부터 성공하는 것은 아닙니다.\n",
      "\n",
      "첫 번째 시도가 실패하더라도 모델을 준비하거나 컨디셔닝하는 다양한 방법을 실험해 보는 것을 두려워하지 마세요.\n",
      "\n",
      "예를 들어 한 개발자는 모델이 더 높은 품질의 응답을 제공하도록 컨디셔닝하기 위해 \"지금까지 수고하셨습니다.\"라는 사용자 메시지를 삽입하여 정확도가 향상되는 것을 발견했습니다.\n",
      "\n",
      "모델의 신뢰성을 높이는 방법에 대한 더 많은 아이디어가 필요하면 [신뢰성을 높이는 기술](../techniques_to_improve_reliability.md) 가이드를 읽어보세요. 이 가이드는 채팅이 아닌 모델을 위해 작성되었지만 많은 원칙이 여전히 적용됩니다.\n",
      "\n",
      "-------------------\n",
      "\n",
      "Not every attempt at engineering conversations will succeed at first.\n",
      "\n",
      "If your first attempts fail, don't be afraid to experiment with different ways of priming or conditioning the model.\n",
      "\n",
      "As an example, one developer discovered an increase in accuracy when they inserted a user message that said \"Great job so far, these have been perfect\" to help condition the model into providing higher quality responses.\n",
      "\n",
      "For more ideas on how to lift the reliability of the models, consider reading our guide on [techniques to increase reliability](../techniques_to_improve_reliability.md). It was written for non-chat models, but many of its principles still apply."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. 토큰 계산하기\n",
      "\n",
      "요청을 제출하면 API가 메시지를 토큰 시퀀스로 변환합니다.\n",
      "\n",
      "사용된 토큰의 개수는\n",
      "- 요청 비용\n",
      "- 응답을 생성하는 데 걸리는 시간\n",
      "- 응답이 최대 토큰 한도에 도달하여 차단되는 경우(`gpt-3.5-turbo`의 경우 4096)\n",
      "\n",
      "2023년 3월 01일부터 다음 함수를 사용하여 메시지 목록이 사용할 토큰 수를 계산할 수 있습니다.\n",
      "\n",
      "------------------\n",
      "\n",
      "When you submit your request, the API transforms the messages into a sequence of tokens.\n",
      "\n",
      "The number of tokens used affects:\n",
      "- the cost of the request\n",
      "- the time it takes to generate the response\n",
      "- when the reply gets cut off from hitting the maximum token limit (4,096 for `gpt-3.5-turbo` or 8,192 for `gpt-4`)\n",
      "\n",
      "You can use the following function to count the number of tokens that a list of messages will use.\n",
      "\n",
      "Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee.\n",
      "\n",
      "Read more about counting tokens in [How to count tokens with tiktoken](How_to_count_tokens_with_tiktoken.ipynb)."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 13,
     "metadata": {},
     "outputs": [],
     "source": [
      "import tiktoken\n",
      "\n",
      "\n",
      "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
      "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
      "    try:\n",
      "        encoding = tiktoken.encoding_for_model(model)\n",
      "    except KeyError:\n",
      "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
      "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
      "    if model == \"gpt-3.5-turbo\":\n",
      "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
      "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
      "    elif model == \"gpt-4\":\n",
      "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
      "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
      "    elif model == \"gpt-3.5-turbo-0301\":\n",
      "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
      "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
      "    elif model == \"gpt-4-0314\":\n",
      "        tokens_per_message = 3\n",
      "        tokens_per_name = 1\n",
      "    else:\n",
      "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
      "    num_tokens = 0\n",
      "    for message in messages:\n",
      "        num_tokens += tokens_per_message\n",
      "        for key, value in message.items():\n",
      "            num_tokens += len(encoding.encode(value))\n",
      "            if key == \"name\":\n",
      "                num_tokens += tokens_per_name\n",
      "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
      "    return num_tokens\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 22,
     "metadata": {},
     "outputs": [],
     "source": [
      "messages = [\n",
      "    {\"role\": \"system\", \"content\": \"귀하는 기업 전문 용어를 일반 영어로 번역하는 패턴을 따르는 유용한 어시스턴트입니다.\"},\n",
      "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"새로운 시너지 효과로 매출 성장에 도움이 될 것입니다.\"},\n",
      "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"함께 잘 작동하면 수익이 증가합니다.\"},\n",
      "    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"더 많은 대역폭이 확보되면 다시 돌아와서 활용도를 높일 수 있는 기회에 대해 논의해 봅시다.\"},\n",
      "    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"더 잘할 수 있는 방법에 대해 나중에 덜 바쁠 때 이야기합시다.\"},\n",
      "    {\"role\": \"user\", \"content\": \"이 늦은 피벗은 고객 결과물을 위해 바다를 끓일 시간이 없다는 것을 의미합니다.\"},\n",
      "\n",
      "#     {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n",
      "#     {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n",
      "#     {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n",
      "#     {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n",
      "#     {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n",
      "#     {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n",
      "]\n"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 24,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "258 prompt tokens counted.\n"
       ]
      }
     ],
     "source": [
      "# example token count from the function defined above\n",
      "print(f\"{num_tokens_from_messages(messages)} prompt tokens counted.\")\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "영어: 126 prompt tokens counted.\n",
      "\n",
      "한글: 258 prompt tokens counted."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": 23,
     "metadata": {},
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": [
        "258 prompt tokens used.\n"
       ]
      }
     ],
     "source": [
      "# example token count from the OpenAI API\n",
      "response = openai.ChatCompletion.create(\n",
      "    model=MODEL,\n",
      "    messages=messages,\n",
      "    temperature=0,\n",
      ")\n",
      "\n",
      "example_messages = [\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"name\": \"example_user\",\n",
      "        \"content\": \"New synergies will help drive top-line growth.\",\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"name\": \"example_assistant\",\n",
      "        \"content\": \"Things working well together will increase revenue.\",\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"name\": \"example_user\",\n",
      "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"system\",\n",
      "        \"name\": \"example_assistant\",\n",
      "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
      "    },\n",
      "    {\n",
      "        \"role\": \"user\",\n",
      "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
      "    },\n",
      "]\n",
      "\n",
      "for model in [\"gpt-3.5-turbo-0301\", \"gpt-4-0314\"]:\n",
      "    print(model)\n",
      "    # example token count from the function defined above\n",
      "    print(f\"{num_tokens_from_messages(example_messages, model)} prompt tokens counted by num_tokens_from_messages().\")\n",
      "    # example token count from the OpenAI API\n",
      "    response = openai.ChatCompletion.create(\n",
      "        model=model,\n",
      "        messages=example_messages,\n",
      "        temperature=0,\n",
      "        max_tokens=1  # we're only counting input tokens here, so let's not waste tokens on the output\n",
      "    )\n",
      "    print(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens counted by the OpenAI API.')\n",
      "    print()\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "영어: 126 prompt tokens counted.\n",
      "\n",
      "한글: 258 prompt tokens counted."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "source": []
    }
   ],
   "metadata": {
    "kernelspec": {
     "display_name": "Python 3 (ipykernel)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.10.10"
    },
    "vscode": {
     "interpreter": {
      "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
     }
    }
   },
   "nbformat": 4,
   "nbformat_minor": 4
  }
  