{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 속도 제한 처리 방법\n",
    "\n",
    "OpenAI API를 반복적으로 호출하면 `429: '요청이 너무 많습니다'` 또는 `RateLimitError`라는 오류 메시지가 표시될 수 있습니다. 이러한 오류 메시지는 API의 속도 제한을 초과하여 발생합니다.\n",
    "\n",
    "이 가이드에서는 전송률 제한 오류를 방지하고 처리하기 위한 팁을 공유합니다.\n",
    "\n",
    "속도 제한 오류를 피하기 위해 병렬 요청을 스로틀링하는 예제 스크립트를 보려면 [api_request_parallel_processor.py](api_request_parallel_processor.py)를 참조하세요.\n",
    "\n",
    "------\n",
    "\n",
    "When you call the OpenAI API repeatedly, you may encounter error messages that say `429: 'Too Many Requests'` or `RateLimitError`. These error messages come from exceeding the API's rate limits.\n",
    "\n",
    "This guide shares tips for avoiding and handling rate limit errors.\n",
    "\n",
    "To see an example script for throttling parallel requests to avoid rate limit errors, see [api_request_parallel_processor.py](api_request_parallel_processor.py).\n",
    "\n",
    "## 속도 제한이 존재하는 이유\n",
    "\n",
    "속도 제한은 API의 일반적인 관행이며, 몇 가지 다른 이유로 설정됩니다.\n",
    "\n",
    "- 첫째, API의 남용이나 오용을 방지하는 데 도움이 됩니다. 예를 들어, 악의적인 공격자가 API에 과부하를 일으키거나 서비스 중단을 유발하기 위해 요청을 폭주시킬 수 있습니다. 속도 제한을 설정하면 이러한 종류의 활동을 방지할 수 있습니다.\n",
    "- 둘째, 속도 제한은 모든 사람이 API에 공정하게 액세스할 수 있도록 보장합니다. 한 사람이나 조직이 지나치게 많은 요청을 하면 다른 모든 사람의 API가 느려질 수 있습니다. OpenAI는 한 사용자가 요청할 수 있는 요청 수를 제한함으로써 모든 사용자가 속도 저하 없이 API를 사용할 수 있도록 보장합니다.\n",
    "- 마지막으로, 속도 제한은 OpenAI가 인프라의 총 부하를 관리하는 데 도움이 될 수 있습니다. API에 대한 요청이 급격히 증가하면 서버에 부담을 주고 성능 문제를 일으킬 수 있습니다. 속도 제한을 설정하면 OpenAI는 모든 사용자에게 원활하고 일관된 환경을 유지할 수 있습니다.\n",
    "\n",
    "속도 제한에 도달하는 것은 실망스러울 수 있지만, 속도 제한은 사용자를 위한 API의 안정적인 운영을 보호하기 위해 존재합니다.\n",
    "\n",
    "---------------\n",
    "\n",
    "Rate limits are a common practice for APIs, and they're put in place for a few different reasons.\n",
    "\n",
    "- First, they help protect against abuse or misuse of the API. For example, a malicious actor could flood the API with requests in an attempt to overload it or cause disruptions in service. By setting rate limits, OpenAI can prevent this kind of activity.\n",
    "- Second, rate limits help ensure that everyone has fair access to the API. If one person or organization makes an excessive number of requests, it could bog down the API for everyone else. By throttling the number of requests that a single user can make, OpenAI ensures that everyone has an opportunity to use the API without experiencing slowdowns.\n",
    "- Lastly, rate limits can help OpenAI manage the aggregate load on its infrastructure. If requests to the API increase dramatically, it could tax the servers and cause performance issues. By setting rate limits, OpenAI can help maintain a smooth and consistent experience for all users.\n",
    "\n",
    "Although hitting rate limits can be frustrating, rate limits exist to protect the reliable operation of the API for its users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default rate limits\n",
    "\n",
    "As of Jan 2023, the default rate limits are:\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "  <tr>\n",
    "    <th></th>\n",
    "    <th>Text Completion &amp; Embedding endpoints</th>\n",
    "    <th>Code &amp; Edit endpoints</th>\n",
    "  </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "  <tr>\n",
    "    <td>Free trial users</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pay-as-you-go users (in your first 48 hours)</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>60 requests / minute</li>\n",
    "            <li>250,000 davinci tokens / minute (and proportionally more for cheaper models)</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pay-as-you-go users (after your first 48 hours)</td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>3,000 requests / minute</li>\n",
    "            <li>250,000 davinci tokens / minute (and proportionally more for cheaper models)</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "    <td>\n",
    "        <ul>\n",
    "            <li>20 requests / minute</li>\n",
    "            <li>150,000 tokens / minute</li>\n",
    "        </ul>\n",
    "    </td>\n",
    "  </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "참고로 1,000토큰은 대략 텍스트 한 페이지에 해당합니다.\n",
    "\n",
    "--------\n",
    "\n",
    "For reference, 1,000 tokens is roughly a page of text.\n",
    "\n",
    "### 기타 속도 제한 리소스\n",
    "\n",
    "다른 리소스에서 OpenAI의 전송률 제한에 대해 자세히 알아보세요:\n",
    "\n",
    "- 가이드: 비율 제한](https://beta.openai.com/docs/guides/rate-limits/overview)\n",
    "- 도움말 센터: API 사용에 요금 제한이 적용되나요?(https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits)\n",
    "- 도움말 센터: 429: '너무 많은 요청' 오류는 어떻게 해결하나요?](https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors)\n",
    "\n",
    "------------------\n",
    "\n",
    "Read more about OpenAI's rate limits in these other resources:\n",
    "\n",
    "- [Guide: Rate limits](https://beta.openai.com/docs/guides/rate-limits/overview)\n",
    "- [Help Center: Is API usage subject to any rate limits?](https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits)\n",
    "- [Help Center: How can I solve 429: 'Too Many Requests' errors?](https://help.openai.com/en/articles/5955604-how-can-i-solve-429-too-many-requests-errors)\n",
    "\n",
    "### 비율 제한 증가 요청하기\n",
    "\n",
    "조직의 요금 한도를 늘리려면 다음 양식을 작성해 주세요:\n",
    "\n",
    "- OpenAI 요금 한도 증액 요청 양식](https://forms.gle/56ZrwXXoxAN1yt6i9)\n",
    "\n",
    "---------------------\n",
    "\n",
    "If you'd like your organization's rate limit increased, please fill out the following form:\n",
    "\n",
    "- [OpenAI Rate Limit Increase Request form](https://forms.gle/56ZrwXXoxAN1yt6i9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전송률 제한 오류 예시\n",
    "\n",
    "API 요청이 너무 빨리 전송되면 속도 제한 오류가 발생합니다. OpenAI Python 라이브러리를 사용하는 경우 다음과 같이 표시됩니다:\n",
    "\n",
    "```\n",
    "RateLimitError: 조직 org-{id}의 기본 코덱스에 대한 분당 요청 수에 대한 속도 제한에 도달했습니다. 제한: 20.000000/분. 현재: 24.000000/분. 문제가 계속되거나 증가를 요청하려면 support@openai.com 으로 문의하세요.\n",
    "```\n",
    "\n",
    "다음은 속도 제한 오류를 트리거하는 예제 코드입니다.\n",
    "\n",
    "-----\n",
    "\n",
    "A rate limit error will occur when API requests are sent too quickly. If using the OpenAI Python library, they will look something like:\n",
    "\n",
    "```\n",
    "RateLimitError: Rate limit reached for default-codex in organization org-{id} on requests per min. Limit: 20.000000 / min. Current: 24.000000 / min. Contact support@openai.com if you continue to have issues or if you’d like to request an increase.\n",
    "```\n",
    "\n",
    "Below is example code for triggering a rate limit error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Rate limit reached for default-codex in organization org-dLiMqqSmLIXuV45cMCo7Pvcj on requests per min. Limit: 20 / min. Please try again in 3s. Contact support@openai.com if you continue to have issues or if you’d like to request an increase.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# request a bunch of completions in a loop\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode-cushman-001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdef magic_function():\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/openai/api_resources/completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TryAgain \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m start \u001b[38;5;241m+\u001b[39m timeout:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[38;5;241m=\u001b[39m \u001b[43mrequestor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[38;5;66;03m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/openai/api_requestor.py:226\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    207\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m     request_timeout: Optional[Union[\u001b[38;5;28mfloat\u001b[39m, Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    215\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    216\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_raw(\n\u001b[1;32m    217\u001b[0m         method\u001b[38;5;241m.\u001b[39mlower(),\n\u001b[1;32m    218\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    224\u001b[0m         request_timeout\u001b[38;5;241m=\u001b[39mrequest_timeout,\n\u001b[1;32m    225\u001b[0m     )\n\u001b[0;32m--> 226\u001b[0m     resp, got_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resp, got_stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/openai/api_requestor.py:619\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    612\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_response_line(\n\u001b[1;32m    613\u001b[0m             line, result\u001b[38;5;241m.\u001b[39mstatus_code, result\u001b[38;5;241m.\u001b[39mheaders, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m parse_stream(result\u001b[38;5;241m.\u001b[39miter_lines())\n\u001b[1;32m    616\u001b[0m     ), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_interpret_response_line\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    625\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    626\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/openai/api_requestor.py:682\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    680\u001b[0m stream_error \u001b[38;5;241m=\u001b[39m stream \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m resp\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream_error \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcode \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_error_response(\n\u001b[1;32m    683\u001b[0m         rbody, rcode, resp\u001b[38;5;241m.\u001b[39mdata, rheaders, stream_error\u001b[38;5;241m=\u001b[39mstream_error\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Rate limit reached for default-codex in organization org-dLiMqqSmLIXuV45cMCo7Pvcj on requests per min. Limit: 20 / min. Please try again in 3s. Contact support@openai.com if you continue to have issues or if you’d like to request an increase."
     ]
    }
   ],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "# request a bunch of completions in a loop\n",
    "for _ in range(100):\n",
    "    openai.Completion.create(\n",
    "        model=\"code-cushman-001\",\n",
    "        prompt=\"def magic_function():\\n\\t\",\n",
    "        max_tokens=10,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 속도 제한 오류를 방지하는 방법\n",
    "\n",
    "### 지수 백오프를 사용하여 재시도하기\n",
    "\n",
    "비율 제한 오류를 피하는 쉬운 방법 중 하나는 무작위 지수 백오프를 사용하여 요청을 자동으로 재시도하는 것입니다. 지수 백오프를 사용하여 재시도한다는 것은 비율 제한 오류가 발생하면 짧은 절전을 수행한 다음 실패한 요청을 다시 시도하는 것을 의미합니다. 요청이 여전히 실패하면 절전 시간이 길어지고 프로세스가 반복됩니다. 이 과정은 요청이 성공하거나 최대 재시도 횟수에 도달할 때까지 계속됩니다.\n",
    "\n",
    "이 접근 방식에는 많은 이점이 있습니다:\n",
    "\n",
    "- 자동 재시도는 충돌이나 데이터 누락 없이 속도 제한 오류에서 복구할 수 있음을 의미합니다.\n",
    "- 지수 백오프를 사용하면 첫 번째 재시도를 빠르게 시도할 수 있으며, 처음 몇 번의 재시도가 실패할 경우 지연 시간이 길어지는 이점을 누릴 수 있습니다.\n",
    "- 지연에 무작위 지터를 추가하면 모든 재시도를 동시에 시도하는 데 도움이 됩니다.\n",
    "\n",
    "실패한 요청은 분당 요청 횟수 제한에 영향을 미치므로 요청을 계속 재전송하는 것은 효과가 없습니다.\n",
    "\n",
    "다음은 몇 가지 해결 방법의 예입니다.\n",
    "\n",
    "-----------\n",
    "\n",
    "One easy way to avoid rate limit errors is to automatically retry requests with a random exponential backoff. Retrying with exponential backoff means performing a short sleep when a rate limit error is hit, then retrying the unsuccessful request. If the request is still unsuccessful, the sleep length is increased and the process is repeated. This continues until the request is successful or until a maximum number of retries is reached.\n",
    "\n",
    "This approach has many benefits:\n",
    "\n",
    "- Automatic retries means you can recover from rate limit errors without crashes or missing data\n",
    "- Exponential backoff means that your first retries can be tried quickly, while still benefiting from longer delays if your first few retries fail\n",
    "- Adding random jitter to the delay helps retries from all hitting at the same time\n",
    "\n",
    "Note that unsuccessful requests contribute to your per-minute limit, so continuously resending a request won’t work.\n",
    "\n",
    "Below are a few example solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 #1: Tenacity 라이브러리 사용\n",
    "\n",
    "[Tenacity](https://tenacity.readthedocs.io/en/latest/)는 거의 모든 것에 재시도 동작을 추가하는 작업을 단순화하기 위해 Python으로 작성된 Apache 2.0 라이센스가 있는 범용 재시도 라이브러리입니다.\n",
    "\n",
    "요청에 기하급수적 백오프를 추가하려면 `tenacity.retry` [decorator](https://peps.python.org/pep-0318/)를 사용하면 됩니다. 다음 예제는 `tenacity.wait_random_exponential` 함수를 사용하여 요청에 무작위 지수 백오프를 추가하는 예제입니다.\n",
    "\n",
    "Tenacity 라이브러리는 타사 도구이며, OpenAI는 안정성이나 보안에 대해 보장하지 않는다는 점에 유의하세요.\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Tenacity](https://tenacity.readthedocs.io/en/latest/) is an Apache 2.0 licensed general-purpose retrying library, written in Python, to simplify the task of adding retry behavior to just about anything.\n",
    "\n",
    "To add exponential backoff to your requests, you can use the `tenacity.retry` [decorator](https://peps.python.org/pep-0318/). The following example uses the `tenacity.wait_random_exponential` function to add random exponential backoff to a request.\n",
    "\n",
    "Note that the Tenacity library is a third-party tool, and OpenAI makes no guarantees about its reliability or security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6tk5HgLHXQbj5JQMdeI8B9iEZFzYo at 0x12027e8e0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" I did a dumb thing.\\n\\nI was at school and I saw a\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1678743207,\n",
       "  \"id\": \"cmpl-6tk5HgLHXQbj5JQMdeI8B9iEZFzYo\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai  # for OpenAI API calls\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def completion_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completion_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 #2: 백오프 라이브러리 사용\n",
    "\n",
    "백오프 및 재시도를 위한 함수 데코레이터를 제공하는 또 다른 라이브러리는 [backoff](https://pypi.org/project/backoff/)입니다.\n",
    "\n",
    "Tenacity와 마찬가지로 백오프 라이브러리는 타사 도구이며, OpenAI는 안정성이나 보안에 대해 보장하지 않습니다.\n",
    "\n",
    "-------------\n",
    "\n",
    "Another library that provides function decorators for backoff and retry is [backoff](https://pypi.org/project/backoff/).\n",
    "\n",
    "Like Tenacity, the backoff library is a third-party tool, and OpenAI makes no guarantees about its reliability or security."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6tkBFXNFwJYSGuVc3Jrlsj1lUbqcY at 0x10817ca90> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" on a sunny but cold winter Wednesday, a southbound Amtrak train rolled out of\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1678743577,\n",
       "  \"id\": \"cmpl-6tkBFXNFwJYSGuVc3Jrlsj1lUbqcY\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import backoff  # for exponential backoff\n",
    "import openai  # for OpenAI API calls\n",
    "\n",
    "\n",
    "@backoff.on_exception(backoff.expo, openai.error.RateLimitError)\n",
    "def completions_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예제 3: 수동 백오프 구현\n",
    "\n",
    "타사 라이브러리를 사용하고 싶지 않다면 자체 백오프 로직을 구현할 수 있습니다.\n",
    "\n",
    "------------\n",
    "\n",
    "If you don't want to use third-party libraries, you can implement your own backoff logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6tkCa6xbwqRLoma35tz8NzMP4EySA at 0x120148360> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" there was anKing known as Henry Kenedy who was the best knitter\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1678743660,\n",
       "  \"id\": \"cmpl-6tkCa6xbwqRLoma35tz8NzMP4EySA\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import random\n",
    "import time\n",
    "\n",
    "import openai\n",
    "\n",
    "# define a retry decorator\n",
    "def retry_with_exponential_backoff(\n",
    "    func,\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    errors: tuple = (openai.error.RateLimitError,),\n",
    "):\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Initialize variables\n",
    "        num_retries = 0\n",
    "        delay = initial_delay\n",
    "\n",
    "        # Loop until a successful response or max_retries is hit or an exception is raised\n",
    "        while True:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            # Retry on specified errors\n",
    "            except errors as e:\n",
    "                # Increment retries\n",
    "                num_retries += 1\n",
    "\n",
    "                # Check if max retries has been reached\n",
    "                if num_retries > max_retries:\n",
    "                    raise Exception(\n",
    "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                    )\n",
    "\n",
    "                # Increment the delay\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "\n",
    "                # Sleep for the delay\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Raise exceptions for any errors not specified\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@retry_with_exponential_backoff\n",
    "def completions_with_backoff(**kwargs):\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "completions_with_backoff(model=\"text-davinci-002\", prompt=\"Once upon a time,\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 속도 제한이 주어졌을 때 일괄 처리 처리량을 최대화하는 방법\n",
    "\n",
    "사용자의 실시간 요청을 처리하는 경우, 백오프 및 재시도는 속도 제한 오류를 방지하면서 지연 시간을 최소화하는 훌륭한 전략입니다.\n",
    "\n",
    "하지만 지연 시간보다 처리량이 더 중요한 대량의 배치 데이터를 처리하는 경우에는 백오프 및 재시도 외에 몇 가지 다른 방법을 사용할 수 있습니다.\n",
    "\n",
    "-------\n",
    "\n",
    "If you're processing real-time requests from users, backoff and retry is a great strategy to minimize latency while avoiding rate limit errors.\n",
    "\n",
    "However, if you're processing large volumes of batch data, where throughput matters more than latency, there are a few other things you can do in addition to backoff and retry.\n",
    "\n",
    "### 선제적으로 요청 사이에 지연 시간 추가하기\n",
    "\n",
    "속도 제한에 도달했다가 백오프하고, 다시 속도 제한에 도달하고, 다시 백오프하는 과정을 반복하면 요청 예산의 상당 부분이 재시도해야 하는 요청에 '낭비'될 수 있습니다. 이렇게 되면 고정된 비율 제한이 주어지면 처리량이 제한됩니다.\n",
    "\n",
    "여기서 한 가지 잠재적인 해결책은 속도 제한을 계산하고 그 역수와 같은 지연을 추가하는 것입니다(예: 속도 제한이 분당 요청 20건인 경우 각 요청에 3~6초의 지연을 추가). 이렇게 하면 한도에 도달하여 요청이 낭비되지 않고 요금 한도 근처에서 운영할 수 있습니다.\n",
    "\n",
    "---------\n",
    "\n",
    "If you are constantly hitting the rate limit, then backing off, then hitting the rate limit again, then backing off again, it's possible that a good fraction of your request budget will be 'wasted' on requests that need to be retried. This limits your processing throughput, given a fixed rate limit.\n",
    "\n",
    "Here, one potential solution is to calculate your rate limit and add a delay equal to its reciprocal (e.g., if your rate limit 20 requests per minute, add a delay of 3–6 seconds to each request). This can help you operate near the rate limit ceiling without hitting it and incurring wasted requests.\n",
    "\n",
    "#### 요청에 지연을 추가하는 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-6tkDlNKG5rCmaRlh7AmEvleikj0Cy at 0x108106520> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \" Mr. Kumar brought radishes from the market.\\n\\nHe was very happy\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1678743733,\n",
       "  \"id\": \"cmpl-6tkDlNKG5rCmaRlh7AmEvleikj0Cy\",\n",
       "  \"model\": \"text-davinci-002\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 16,\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"total_tokens\": 21\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import time\n",
    "import openai\n",
    "\n",
    "# Define a function that adds a delay to a Completion API call\n",
    "def delayed_completion(delay_in_seconds: float = 1, **kwargs):\n",
    "    \"\"\"Delay a completion by a specified amount of time.\"\"\"\n",
    "\n",
    "    # Sleep for the delay\n",
    "    time.sleep(delay_in_seconds)\n",
    "\n",
    "    # Call the Completion API and return the result\n",
    "    return openai.Completion.create(**kwargs)\n",
    "\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "rate_limit_per_minute = 20\n",
    "delay = 60.0 / rate_limit_per_minute\n",
    "\n",
    "delayed_completion(\n",
    "    delay_in_seconds=delay,\n",
    "    model=\"text-davinci-002\",\n",
    "    prompt=\"Once upon a time,\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요청 일괄 처리\n",
    "\n",
    "OpenAI API에는 분당 요청 수와 분당 토큰 수에 대한 별도의 제한이 있습니다.\n",
    "\n",
    "분당 요청 수 제한에 도달했지만 분당 토큰 수에 여유가 있는 경우, 여러 작업을 각 요청에 일괄 처리하여 처리량을 늘릴 수 있습니다. 이렇게 하면 특히 소규모 모델에서 분당 더 많은 토큰을 처리할 수 있습니다.\n",
    "\n",
    "프롬프트 일괄 전송은 일반 API 호출과 완전히 동일하게 작동하지만, 단일 문자열 대신 '프롬프트' 매개변수에 문자열 목록을 전달한다는 점이 다릅니다.\n",
    "\n",
    "**경고: 응답 객체가 프롬프트 순서대로 완료를 반환하지 않을 수 있으므로 항상 `index` 필드를 사용하여 응답을 프롬프트에 다시 일치시켜야 합니다.\n",
    "\n",
    "--------\n",
    "\n",
    "The OpenAI API has separate limits for requests per minute and tokens per minute.\n",
    "\n",
    "If you're hitting the limit on requests per minute, but have headroom on tokens per minute, you can increase your throughput by batching multiple tasks into each request. This will allow you to process more tokens per minute, especially with the smaller models.\n",
    "\n",
    "Sending in a batch of prompts works exactly the same as a normal API call, except that pass in a list of strings to `prompt` parameter instead of a single string.\n",
    "\n",
    "**Warning:** the response object may not return completions in the order of the prompts, so always remember to match responses back to prompts using the `index` field.\n",
    "\n",
    "#### 일괄 처리하지 않은 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옛날 옛적에, 숙녀가 머리 빗물\n",
      "옛날 옛적에,\n",
      "우리 마을에는\n",
      "\n",
      "옛날 옛적에, 동네에는 우리할\n",
      "옛날 옛적에, 젊은이들이 가릴\n",
      "옛날 옛적에,\n",
      "\n",
      "옛날 옛적에는\n",
      "옛날 옛적에,\n",
      "\n",
      "교회가 중요했\n",
      "옛날 옛적에, 한 나라 곳곳을\n",
      "옛날 옛적에, 고등학교 학생\n",
      "옛날 옛적에, 장군은 기습을 \n",
      "옛날 옛적에, 기사들은 매우 열\n",
      "옛날 옛적에, 모니터는 대부분\n",
      "옛날 옛적에, 특히 일제나 다\n",
      "옛날 옛적에, 궁궐 안에는 총\n",
      "옛날 옛적에, 길이 멀리서는\n",
      "옛날 옛적에,\n",
      "\n",
      "옛날 옛적에는\n",
      "옛날 옛적에, 한 나라 속 금빛 \n",
      "옛날 옛적에, 훗날 옛적에는 \n",
      "옛날 옛적에, 나는 작은 골목\n",
      "옛날 옛적에, 고기를 씻는 방\n",
      "옛날 옛적에, 금빛 빵 가게가\n",
      "옛날 옛적에, 사람들이\n",
      "\n",
      "옛날\n",
      "옛날 옛적에, 개미가 개구리\n",
      "옛날 옛적에, 고향 속 명물인 뿔\n",
      "옛날 옛적에,\n",
      "\n",
      "옛날 옛날에는\n",
      "옛날 옛적에, 고대 명목 개념\n",
      "옛날 옛적에, '가깝길'은 없다\n",
      "옛날 옛적에, 그리운 추억들\n",
      "\n",
      "옛날 옛적에, 이상한 이야기가\n",
      "옛날 옛적에,\n",
      "큰가지 길이 있\n",
      "옛날 옛적에, 아마존 우리가\n",
      "옛날 옛적에, 나의 이름을 불러\n",
      "옛날 옛적에, 내 손끝 잡고 갔\n",
      "옛날 옛적에, 생활 그리고 사\n",
      "옛날 옛적에,\n",
      "사람들은 멍멍이\n",
      "옛날 옛적에, 그들 사이에 한 남\n",
      "옛날 옛적에, 한 산골에 가장\n",
      "옛날 옛적에, 한 농부가 옆밭\n",
      "옛날 옛적에,\n",
      "길 가는 난민에\n",
      "옛날 옛적에,\n",
      ">\n",
      "> 스스로 혼자\n",
      "옛날 옛적에, 방금, 애매\n",
      "\n",
      "**옛\n",
      "옛날 옛적에, 5살 어린 엔딩이\n",
      "옛날 옛적에, 먼지가 많은 창문\n",
      "옛날 옛적에, 말로는 재배가\n",
      "옛날 옛적에, 가난한 일꾼이 무\n",
      "옛날 옛적에,\n",
      "\n",
      "고대 시대 때에\n",
      "옛날 옛적에, 나는 학교에 갔\n",
      "옛날 옛적에,\n",
      "\n",
      "옛날 옛적에는\n",
      "옛날 옛적에, 그 밤하늘에 떠\n",
      "옛날 옛적에, 아이 오 드 테이 \n",
      "옛날 옛적에, 내 늙장이가 다음\n",
      "옛날 옛적에, 아들자식이 손님\n",
      "옛날 옛적에,\n",
      "  어느 승려가 나\n",
      "옛날 옛적에, 사람들은\n",
      "\n",
      "집에\n",
      "옛날 옛적에, 산다는 말은\n",
      "어떤\n",
      "옛날 옛적에,\n",
      "\n",
      "나라 가는 노\n",
      "옛날 옛적에, 모두 이야기 하\n",
      "옛날 옛적에, 옛날 사람들은 이\n",
      "옛날 옛적에, 대한민국 사람들\n",
      "옛날 옛적에, 실용주의 사회들\n",
      "옛날 옛적에, 대한민국의 백지\n",
      "옛날 옛적에, 하늘 같이 높게\n",
      "\n",
      "\n",
      "옛날 옛적에, 손 없는 기사는\n",
      "옛날 옛적에, 직접 코딩을 하는\n",
      "옛날 옛적에, 사람들이 작은 개\n",
      "옛날 옛적에, 나의 나라\"\n",
      "}, {\n",
      "    \"song\":\n",
      "옛날 옛적에, 그려진 로마 유\n",
      "옛날 옛적에, 명맥은 한국의 역\n",
      "옛날 옛적에, 땅콩 다방이 인기\n",
      "옛날 옛적에, 예수는 지금도\n",
      "옛날 옛적에, 길건너 꽃들이 \n",
      "옛날 옛적에, 먼 오래된 곳에\n",
      "\n",
      "옛날 옛적에, 있었던 \n",
      "누군가\n",
      "옛날 옛적에,\n",
      "신이 나시는 부\n",
      "옛날 옛적에,\n",
      "호랑이 타고 있\n",
      "옛날 옛적에, 고인물들은 자신\n",
      "옛날 옛적에,\n",
      "헐리우드에서\n",
      "옛날 옛적에, 한 아이가 생각나\n",
      "옛날 옛적에, 한나라 살아오\n",
      "옛날 옛적에, 당신 주변에 누\n",
      "옛날 옛적에, 고대 사람들은 자\n",
      "옛날 옛적에, 인간들은 평화\n",
      "옛날 옛적에, 한 소년이 강가\n",
      "옛날 옛적에, 노인이라면 다 먹\n",
      "옛날 옛적에, 5호 손님께서 당\n",
      "옛날 옛적에, 종료하는게 아니\n",
      "옛날 옛적에,\n",
      "\n",
      "과거에는 많은\n",
      "옛날 옛적에, 몇몇 연구원들은\n",
      "옛날 옛적에, 나는 작은 골목\n",
      "옛날 옛적에, 하나씩 모든 가\n",
      "옛날 옛적에, 토르는 한 전설\n",
      "옛날 옛적에, 금방에는 모험\n",
      "옛날 옛적에, 사람들은 테두리\n",
      "옛날 옛적에, 농부는 밭에서 \n",
      "옛날 옛적에, 농가들은 자연의\n",
      "옛날 옛적에, 주먹과 발로 치\n",
      "옛날 옛적에, 한 남자가 한 여\n",
      "옛날 옛적에, 신문 기사 보고 \n",
      "옛날 옛적에, 한 나라 어느 곳\n",
      "옛날 옛적에,\n",
      "\n",
      "종이로 쓰고 보\n",
      "옛날 옛적에, 지구는 하늘처\n"
     ]
    }
   ],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "\n",
    "num_stories = 10\n",
    "prompt = \"Once upon a time,\"\n",
    "prompt = \"옛날 옛적에,\"\n",
    "\n",
    "# serial example, with one story completion per request\n",
    "for _ in range(num_stories):\n",
    "    response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "#         model=\"davinci\",\n",
    "#         model=\"curie\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=200,\n",
    "    )\n",
    "\n",
    "    # print story\n",
    "    print(prompt + response.choices[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "옛날 옛적에, 옛날 옛적에 아버지가 목장에 양 한 마리를 가지고 갔어요. 그는 매일 양과 함께 저녁 때 먹을 채소를 심어 놓았습니다. 이 채소는 일년 내내 마른 계절이나 비오는 겨울에\n",
      "---------------------\n",
      "옛날 옛적에, 용사가 산 위로 올라갔다. 한 병기를 차고 노래를 불러도록 하길 바랬던 백성들이 어느새 군량에 모이기 시작했다. 용사 앞에 무리가 펼쳐지자 그의 눈과 가슴 모두 무척 충\n",
      "---------------------\n",
      "옛날 옛적에, 빨간 소녀 이렇게 말했다 \n",
      "\n",
      "\"오늘도 에세이를 써봐야겠다. 내가 무슨 생각이 있든, 내가 무엇을 놀라게 할 수 있든, 이 글에서 녹아 내는 감정이 누구를 생각하게 할까? 그것\n",
      "---------------------\n",
      "옛날 옛적에, 쏘레(호박 라디오)에 호피가 게임을 개발하였다. 호피가 개발한 게임은 원작으로써 밀다르 게임이라는 이름의 동물 가족을 연결하는 게임을 의미하였다. 이 게임은 다른\n",
      "---------------------\n",
      "옛날 옛적에, 사람들은 천주교가 전해주는 믿음이 최고의 진리라고 믿었습니다. 그들은 그 내용을 신중하게 추론하고 생각하였으며, 이것을 지킬 것이라 생각하였습니다. 그렇게 그들은 사람\n",
      "---------------------\n",
      "옛날 옛적에, 우리 떤 남자 였다\n",
      "\n",
      "옛날 옛적에 우리는 다섯 남자가 있었다. 그 남자들은 힘찬 용맹과 의지로 자신들의 삶을 이뤄내기 위해 노력하는 독특한 문화가 있었고, 그들의 삶에는\n",
      "---------------------\n",
      "옛날 옛적에, <strong>못 먹게 만든 배알이 길들인다</strong>라는 속담이 있습니다.\n",
      "이 속담은 상황이 어려워졌을 때를 의미하며, 하나의 노력이라도 하면 나아지게 된다는 뜻을 이끌어 냅니다. 이 속담에 따\n",
      "---------------------\n",
      "옛날 옛적에, 한 산 꼭대기에 이성 깊이\n",
      "우리 둘이 다툼이었죠\n",
      "별들 소리도 나오고 바람도 불었죠\n",
      "그런 밤이 오면 우리는 잠들었죠\n",
      "\n",
      "길 건너 사람들은 모두 잊혀지고 버려지고\n",
      "나만\n",
      "---------------------\n",
      "옛날 옛적에, 한 나라가 다른 나라와 전쟁을 벌였습니다. 그 때 각 나라는 자신들만의 무기를 개발하여 전쟁에 승리하고자 했습니다. 그 중 그 시대에 가장 신성하게 느껴지는 무기가\n",
      "---------------------\n",
      "옛날 옛적에, 사과는 그리스 전쟁 중간에 전해받은 음식으로, 사과는 그들에게 스타일과 맛의 차이를 제공하였다고 전해진다. 이는 사과가 그때로부터 전해져 오는 음식이라는 것을 의미한\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "import openai  # for making OpenAI API requests\n",
    "\n",
    "\n",
    "num_stories = 10\n",
    "# prompts = [\"Once upon a time,\"] * num_stories\n",
    "prompts = [\"옛날 옛적에,\"] * num_stories\n",
    "\n",
    "# batched example, with 10 stories completions per request\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "#     model=\"davinci\",\n",
    "#     model=\"curie\",\n",
    "    prompt=prompts,\n",
    "    max_tokens=200,\n",
    ")\n",
    "\n",
    "# match completions to prompts by index\n",
    "stories = [\"\"] * len(prompts)\n",
    "for choice in response.choices:\n",
    "    stories[choice.index] = prompts[choice.index] + choice.text\n",
    "\n",
    "# print stories\n",
    "for story in stories:\n",
    "    print(story)\n",
    "    print(\"---------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 병렬 처리 스크립트 예시\n",
    "\n",
    "대량의 API 요청을 병렬 처리하기 위한 예제 스크립트를 작성했습니다: [api_request_parallel_processor.py](api_request_parallel_processor.py).\n",
    "\n",
    "이 스크립트에는 몇 가지 편리한 기능이 결합되어 있습니다:\n",
    "- 대규모 작업으로 인한 메모리 부족을 방지하기 위해 파일에서 요청을 스트리밍합니다.\n",
    "- 처리량을 최대화하기 위해 요청을 동시에 수행합니다.\n",
    "- 요청과 토큰 사용량을 모두 스로틀하여 속도 제한을 준수합니다.\n",
    "- 데이터 누락을 방지하기 위해 실패한 요청을 재시도합니다.\n",
    "- 오류를 기록하여 요청 문제를 진단합니다.\n",
    "\n",
    "있는 그대로 사용하거나 필요에 맞게 자유롭게 수정하세요.\n",
    "\n",
    "---------------------\n",
    "\n",
    "We've written an example script for parallel processing large quantities of API requests: [api_request_parallel_processor.py](api_request_parallel_processor.py).\n",
    "\n",
    "The script combines some handy features:\n",
    "- Streams requests from file, to avoid running out of memory for giant jobs\n",
    "- Makes requests concurrently, to maximize throughput\n",
    "- Throttles both request and token usage, to stay under rate limits\n",
    "- Retries failed requests, to avoid missing data\n",
    "- Logs errors, to diagnose problems with requests\n",
    "\n",
    "Feel free to use it as is or modify it to suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
