{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e67f200",
   "metadata": {},
   "source": [
    "# 지식창고로 함수를 사용하는 방법\n",
    "\n",
    "이 노트북에서는 지식창고에 액세스할 수 있는 에이전트와 사용자 요구 사항에 따라 호출할 수 있는 두 개의 함수를 만들어 [인수 생성]('How_to_generate_function_arguments_with_chat_models.ipynb') 노트북의 개념을 기반으로 합니다.\n",
    "\n",
    "학술 주제에 대한 질문에 답하기 위해 arXiv의 데이터를 사용하는 에이전트를 만들어 보겠습니다. 이 에이전트에는 다음과 같은 두 가지 함수가 있습니다: \n",
    "\n",
    "- **get_articles:** 주제에 대한 arXiv 논문을 가져와 링크와 함께 사용자에게 요약하는 함수 \n",
    "- **read_article_and_summarize**: 이 기능은 이전에 검색한 논문 중 하나를 가져와 전체 내용을 읽고 핵심 주장, 증거 및 결론을 요약합니다.\n",
    "\n",
    "이렇게 하면 여러 서비스 중에서 선택할 수 있고 첫 번째 함수의 데이터 중 일부가 두 번째 함수에서 계속 사용되는 다기능 워크플로우에 익숙해질 수 있습니다.\n",
    "\n",
    "## 연습\n",
    "\n",
    "이 쿡북에서는 다음과 같은 워크플로우를 안내합니다:\n",
    "\n",
    "- **검색 유틸리티:** 답변을 얻기 위해 arXiv에 액세스하는 두 가지 함수 만들기 \n",
    "- **에이전트 구성:** 함수의 필요성을 평가하고, 필요한 경우 해당 함수를 호출하여 결과를 에이전트에게 다시 제시하는 에이전트 동작 구축 \n",
    "- **arXiv 대화:** 이 모든 것을 실시간 대화에 통합하기."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80e71f33",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (1.10.1)\n",
      "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from scipy) (1.24.3)\n",
      "Requirement already satisfied: tenacity in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (8.2.2)\n",
      "Collecting tiktoken==0.3.3\n",
      "  Using cached tiktoken-0.3.3-cp310-cp310-macosx_11_0_arm64.whl (706 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from tiktoken==0.3.3) (2023.6.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from tiktoken==0.3.3) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.3.3) (2023.5.7)\n",
      "Installing collected packages: tiktoken\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.4.0\n",
      "    Uninstalling tiktoken-0.4.0:\n",
      "      Successfully uninstalled tiktoken-0.4.0\n",
      "Successfully installed tiktoken-0.3.3\n",
      "Requirement already satisfied: termcolor in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: openai in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests>=2.20->openai) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: requests in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from requests) (2023.5.7)\n",
      "Requirement already satisfied: arxiv in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (1.4.7)\n",
      "Requirement already satisfied: feedparser in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from arxiv) (6.0.10)\n",
      "Requirement already satisfied: sgmllib3k in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from feedparser->arxiv) (1.0.0)\n",
      "Requirement already satisfied: pandas in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: PyPDF2 in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (3.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/wlkim/anaconda3/envs/openai-cookbook/lib/python3.10/site-packages (4.65.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install tenacity\n",
    "!pip install tiktoken==0.3.3\n",
    "!pip install termcolor \n",
    "!pip install openai\n",
    "!pip install requests\n",
    "!pip install arxiv\n",
    "!pip install pandas\n",
    "!pip install PyPDF2\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dab872c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import ast\n",
    "import concurrent\n",
    "from csv import writer\n",
    "from IPython.display import display, Markdown, Latex\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from scipy import spatial\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "import tiktoken\n",
    "from tqdm import tqdm\n",
    "from termcolor import colored\n",
    "\n",
    "GPT_MODEL = \"gpt-3.5-turbo-16k-0613\"\n",
    "# GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e47962",
   "metadata": {},
   "source": [
    "## 검색 유틸리티\n",
    "\n",
    "먼저 두 가지 기능을 뒷받침할 몇 가지 유틸리티를 설정하겠습니다.\n",
    "\n",
    "다운로드한 논문은 디렉토리에 저장됩니다(여기서는 ```./data/papers```을 사용합니다). 다운로드한 논문의 임베딩과 세부 정보를 저장하여 ```summarize_text```를 사용하여 검색할 수 있도록 ``arxiv_library.csv`` 파일을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2de5d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a directory to store downloaded papers\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
    "paper_dir_filepath = \"./data/arxiv_library.csv\"\n",
    "\n",
    "# Generate a blank dataframe where we can store downloaded files\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "57217b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    response = openai.Embedding.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=5):\n",
    "    \"\"\"This function gets the top_k articles based on a user's query, sorted by relevance.\n",
    "    It also downloads the files and stores them in arxiv_library.csv to be retrieved by the read_article_and_summarize.\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query, max_results=top_k, sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in search.results():\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "\n",
    "        # Taking the first url provided\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # Store references in library file\n",
    "        response = embedding_request(text=result.title)\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            response[\"data\"][0][\"embedding\"],\n",
    "        ]\n",
    "\n",
    "        # Write to file\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dda02bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Proximal Policy Optimization and its Dynamic Version for Sequence Generation',\n",
       " 'summary': 'In sequence generation task, many works use policy gradient for model\\noptimization to tackle the intractable backpropagation issue when maximizing\\nthe non-differentiable evaluation metrics or fooling the discriminator in\\nadversarial learning. In this paper, we replace policy gradient with proximal\\npolicy optimization (PPO), which is a proved more efficient reinforcement\\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\\nperformance.',\n",
       " 'article_url': 'http://arxiv.org/abs/1808.07982v1',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/1808.07982v1'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"ppo reinforcement learning\")\n",
    "result_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bca725b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Proximal Policy Optimization and its Dynamic Version for Sequence Generation',\n",
       "  'summary': 'In sequence generation task, many works use policy gradient for model\\noptimization to tackle the intractable backpropagation issue when maximizing\\nthe non-differentiable evaluation metrics or fooling the discriminator in\\nadversarial learning. In this paper, we replace policy gradient with proximal\\npolicy optimization (PPO), which is a proved more efficient reinforcement\\nlearning algorithm, and propose a dynamic approach for PPO (PPO-dynamic). We\\ndemonstrate the efficacy of PPO and PPO-dynamic on conditional sequence\\ngeneration tasks including synthetic experiment and chit-chat chatbot. The\\nresults show that PPO and PPO-dynamic can beat policy gradient by stability and\\nperformance.',\n",
       "  'article_url': 'http://arxiv.org/abs/1808.07982v1',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/1808.07982v1'},\n",
       " {'title': 'CIM-PPO:Proximal Policy Optimization with Liu-Correntropy Induced Metric',\n",
       "  'summary': \"As an algorithm based on deep reinforcement learning, Proximal Policy\\nOptimization (PPO) performs well in many complex tasks and has become one of\\nthe most popular RL algorithms in recent years. According to the mechanism of\\npenalty in surrogate objective, PPO can be divided into PPO with KL Divergence\\n(KL-PPO) and PPO with Clip function(Clip-PPO). Clip-PPO is widely used in a\\nvariety of practical scenarios and has attracted the attention of many\\nresearchers. Therefore, many variations have also been created, making the\\nalgorithm better and better. However, as a more theoretical algorithm, KL-PPO\\nwas neglected because its performance was not as good as CliP-PPO. In this\\narticle, we analyze the asymmetry effect of KL divergence on PPO's objective\\nfunction , and give the inequality that can indicate when the asymmetry will\\naffect the efficiency of KL-PPO. Proposed PPO with Correntropy Induced Metric\\nalgorithm(CIM-PPO) that use the theory of correntropy(a symmetry metric method\\nthat was widely used in M-estimation to evaluate two distributions'\\ndifference)and applied it in PPO. Then, we designed experiments based on\\nOpenAIgym to test the effectiveness of the new algorithm and compare it with\\nKL-PPO and CliP-PPO.\",\n",
       "  'article_url': 'http://arxiv.org/abs/2110.10522v2',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2110.10522v2'},\n",
       " {'title': 'A2C is a special case of PPO',\n",
       "  'summary': \"Advantage Actor-critic (A2C) and Proximal Policy Optimization (PPO) are\\npopular deep reinforcement learning algorithms used for game AI in recent\\nyears. A common understanding is that A2C and PPO are separate algorithms\\nbecause PPO's clipped objective appears significantly different than A2C's\\nobjective. In this paper, however, we show A2C is a special case of PPO. We\\npresent theoretical justifications and pseudocode analysis to demonstrate why.\\nTo validate our claim, we conduct an empirical experiment using\\n\\\\texttt{Stable-baselines3}, showing A2C and PPO produce the \\\\textit{exact} same\\nmodels when other settings are controlled.\",\n",
       "  'article_url': 'http://arxiv.org/abs/2205.09123v1',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2205.09123v1'},\n",
       " {'title': 'Proximal Policy Optimization via Enhanced Exploration Efficiency',\n",
       "  'summary': \"Proximal policy optimization (PPO) algorithm is a deep reinforcement learning\\nalgorithm with outstanding performance, especially in continuous control tasks.\\nBut the performance of this method is still affected by its exploration\\nability. For classical reinforcement learning, there are some schemes that make\\nexploration more full and balanced with data exploitation, but they can't be\\napplied in complex environments due to the complexity of algorithm. Based on\\ncontinuous control tasks with dense reward, this paper analyzes the assumption\\nof the original Gaussian action exploration mechanism in PPO algorithm, and\\nclarifies the influence of exploration ability on performance. Afterward,\\naiming at the problem of exploration, an exploration enhancement mechanism\\nbased on uncertainty estimation is designed in this paper. Then, we apply\\nexploration enhancement theory to PPO algorithm and propose the proximal policy\\noptimization algorithm with intrinsic exploration module (IEM-PPO) which can be\\nused in complex environments. In the experimental parts, we evaluate our method\\non multiple tasks of MuJoCo physical simulator, and compare IEM-PPO algorithm\\nwith curiosity driven exploration algorithm (ICM-PPO) and original algorithm\\n(PPO). The experimental results demonstrate that IEM-PPO algorithm needs longer\\ntraining time, but performs better in terms of sample efficiency and cumulative\\nreward, and has stability and robustness.\",\n",
       "  'article_url': 'http://arxiv.org/abs/2011.05525v1',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2011.05525v1'},\n",
       " {'title': 'Neural PPO-Clip Attains Global Optimality: A Hinge Loss Perspective',\n",
       "  'summary': 'Policy optimization is a fundamental principle for designing reinforcement\\nlearning algorithms, and one example is the proximal policy optimization\\nalgorithm with a clipped surrogate objective (PPO-Clip), which has been\\npopularly used in deep reinforcement learning due to its simplicity and\\neffectiveness. Despite its superior empirical performance, PPO-Clip has not\\nbeen justified via theoretical proof up to date. In this paper, we establish\\nthe first global convergence rate of PPO-Clip under neural function\\napproximation. We identify the fundamental challenges of analyzing PPO-Clip and\\naddress them with the two core ideas: (i) We reinterpret PPO-Clip from the\\nperspective of hinge loss, which connects policy improvement with solving a\\nlarge-margin classification problem with hinge loss and offers a generalized\\nversion of the PPO-Clip objective. (ii) Based on the above viewpoint, we\\npropose a two-step policy improvement scheme, which facilitates the convergence\\nanalysis by decoupling policy search from the complex neural policy\\nparameterization with the help of entropic mirror descent and a\\nregression-based policy update scheme. Moreover, our theoretical results\\nprovide the first characterization of the effect of the clipping mechanism on\\nthe convergence of PPO-Clip. Through experiments, we empirically validate the\\nreinterpretation of PPO-Clip and the generalized objective with various\\nclassifiers on various RL benchmark tasks.',\n",
       "  'article_url': 'http://arxiv.org/abs/2110.13799v4',\n",
       "  'pdf_url': 'http://arxiv.org/pdf/2110.13799v4'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "140a3362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT',\n",
       " 'summary': 'In this paper, we aim to develop a large language model (LLM) with the\\nreasoning ability on complex graph data. Currently, LLMs have achieved very\\nimpressive performance on various natural language learning tasks, extensions\\nof which have also been applied to study the vision tasks with multi-modal\\ndata. However, when it comes to the graph learning tasks, existing LLMs present\\nvery serious flaws due to their several inherited weaknesses in performing\\n{multi-step logic reasoning}, {precise mathematical calculation} and\\n{perception about the spatial and temporal factors}.\\n  To address such challenges, in this paper, we will investigate the\\nprinciples, methodologies and algorithms to empower existing LLMs with graph\\nreasoning ability, which will have tremendous impacts on the current research\\nof both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer\\nmodels, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer)\\nframework to teach LLMs themselves with prompts augmented by ChatGPT to use\\nexternal graph reasoning API tools. Specifically, we will investigate to teach\\nGraph-ToolFormer to handle various graph data reasoning tasks in this paper,\\nincluding both (1) very basic graph data loading and graph property reasoning\\ntasks, ranging from simple graph order and size to the graph diameter and\\nperiphery, and (2) more advanced reasoning tasks on real-world graph data, such\\nas bibliographic networks, protein molecules, sequential recommender systems,\\nsocial networks and knowledge graphs.',\n",
       " 'article_url': 'http://arxiv.org/abs/2304.11116v3',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2304.11116v3'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test that the search is working\n",
    "result_output = get_articles(\"toolformer\")\n",
    "result_output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11675627",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7211df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"Takes a filepath to a PDF and returns a string of the PDF's contents\"\"\"\n",
    "    # creating a pdf reader object\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "# Split a text into smaller chunks of size n, preferably ending at the end of a sentence\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"Returns successive n-sized chunks from provided text.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # Find the nearest end of sentence within a range of 0.5 * n and 1.5 * n tokens\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # Decode the tokens and check for full stop or newline\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # If no end of sentence found, use n tokens as the chunk size\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"This function applies a prompt to some input content. In this case it returns a summarize chunk of text\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"This function does the following:\n",
    "    - Reads in the arxiv_library.csv file in including the embeddings\n",
    "    - Finds the closest file to the user's query\n",
    "    - Scrapes the text out of the file and chunks it\n",
    "    - Summarizes each chunk in parallel\n",
    "    - Does one final summary and returns this to the user\"\"\"\n",
    "\n",
    "    # A prompt to dictate how the recursive summarizations should approach the input paper\n",
    "    summary_prompt = \"\"\"Summarize this text from an academic paper. Extract any key points with reasoning.\\n\\nContent:\"\"\"\n",
    "\n",
    "    # If the library is empty (no searches have been performed yet), we perform one and download the results\n",
    "    library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    if len(library_df) == 0:\n",
    "        print(\"No papers searched yet, downloading first.\")\n",
    "        get_articles(query)\n",
    "        print(\"Papers downloaded, continuing\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath).reset_index()\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "    print(\"Chunking text from paper\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # Initialise tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # Chunk up the document into 1500 token chunks\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"Summarizing each chunk of text\")\n",
    "\n",
    "    # Parallel process the summaries\n",
    "    with concurrent.futures.ThreadPoolExecutor(\n",
    "        max_workers=len(text_chunks)\n",
    "    ) as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_chunk, chunk, summary_prompt)\n",
    "            for chunk in text_chunks\n",
    "        ]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # Final summary\n",
    "    print(\"Summarizing into overall summary\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=GPT_MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Write a summary in Korean collated from this collection of key points extracted from an academic paper. Must use Korean.\n",
    "                        The summary should highlight the core argument, conclusions and evidence, and answer the user's query.\n",
    "                        User query: {query}\n",
    "                        The summary should be structured in bulleted lists following the headings Core Argument, Evidence, and Conclusions.\n",
    "                        Key points:\\n{results}\\nSummary:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "898b94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 5/5 [00:07<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    }
   ],
   "source": [
    "# Test the summarize_text function works\n",
    "chat_test_response = summarize_text(\"PPO reinforcement learning sequence generation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c715f60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "핵심 주장:\n",
      "- 이 논문은 시퀀스 생성 작업에서 Proximal Policy Optimization (PPO)의 사용에 대해 논의한다.\n",
      "- PPO-dynamic이라는 수정된 버전의 PPO를 제안하고, 이를 시퀀스 생성 모델 최적화에 일반적으로 사용되는 Policy Gradient와 비교한다.\n",
      "- 결과는 PPO와 PPO-dynamic이 안정성과 성능 측면에서 Policy Gradient보다 우수함을 보여준다.\n",
      "\n",
      "증거:\n",
      "- PPO-dynamic은 합성 계수 작업에서 다른 알고리즘보다 더 높은 정확도를 보여준다.\n",
      "- PPO-dynamic은 PPO보다 더 빠른 학습 진행을 보여준다.\n",
      "- OpenSubtitles 데이터셋을 사용한 chit-chat chatbot 작업에서 PPO-dynamic은 다른 알고리즘과 비교되며 BLEU-2 점수를 사용하여 평가된다. 결과는 제공되지 않는다.\n",
      "\n",
      "결론:\n",
      "- MLE, REINFORCE 및 PPO 세 가지 최적화 방법은 비슷한 성능을 보이지만, PPO-dynamic은 다른 방법보다 약간 더 높은 BLEU-2 점수를 달성한다.\n",
      "- PPO와 PPO-dynamic의 학습 진행은 Policy Gradient보다 안정적이며, PPO-dynamic은 더 빠르게 수렴한다.\n",
      "- 결과는 PPO가 시퀀스 학습에 더 적합하며, REINFORCE 사용 시 발생하는 높은 분산 문제를 개선할 수 있다는 것을 시사한다.\n",
      "- PPO-dynamic은 최적화 과정을 더 개선한다.\n",
      "\n",
      "증거:\n",
      "1. 논문은 두 확률 분포 사이의 Kullback-Leibler (KL) 발산을 계산하는 수학적 식 (식 1)을 제시한다.\n",
      "2. 논문은 이전 확률 분포와 새로운 확률 분포의 비율로 정의된 상수 β(x)를 소개한다 (식 2).\n",
      "3. 논문은 식 1의 β(x) 항을 식 3의 식으로 대체하여 KL 발산에 대한 식 (식 4)을 유도한다.\n",
      "4. 논문은 α(a)가 1보다 작다고 가정하고, ln(1 + α(a)) 및 ln(1 - α(a)Pold(a)/(1 - Pold(a)))를 근사하기 위해 Taylor 전개를 사용한다 (식 5).\n",
      "5. 식 4의 항들을 식 5의 식으로 대체함으로써 KL 발산에 대한 단순화된 식 (식 6)을 얻는다.\n",
      "6. 논문은 PPO를 P(a)/Pold(a)로 제한함으로써 PPO를 제약하는 경우, α(a)는 √(2δ/(1 - Pold(a))/Pold(a)) 또는 √(2δ/(1/Pold(a) - 1))로 제한되어야 한다고 결론 짓는다 (식 7).\n",
      "7. 논문은 PPO와 PPO-dynamic 알고리즘에 대한 의사 코드도 포함한다 (알고리즘 1).\n",
      "8. 논문은 베이스라인의 어드밴티지 함수 훈련 및 다른 알고리즘에 사용된 하이퍼파라미터를 포함한 실험 설정을 설명한다.\n",
      "9. 논문은 REINFORCE 및 PPO 방법을 사용한 계수 작업의 첫 번째 출력 분포를 비교하여 결과를 제시한다 (그림 1).\n",
      "10. 논문은 입력 문장 길이에 따른 출력 분포의 분산을 평가하며, PPO-dynamic은 입력에 따라 분포를 조정할 수 있음을 보여준다 (표 1).\n",
      "11. 논문은 REINFORCE, PPO 및 PPO-dynamic 알고리즘을 사용한 훈련된 chit-chat chatbot의 일부 출력 예시를 제시한다 (표 2).\n",
      "12. 논문은 순환 신경망과 Proximal Policy Optimization 알고리즘을 사용한 시퀀스 레벨 훈련에 대한 관련 연구에 대한 참고문헌을 포함한다.\n"
     ]
    }
   ],
   "source": [
    "print(chat_test_response[\"choices\"][0][\"message\"][\"content\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab07e98",
   "metadata": {},
   "source": [
    "## 에이전트 구성\n",
    "\n",
    "이 단계에서는 API를 여러 차례 사용할 수 있도록 지원하는 ```Conversation``` 클래스와 ```ChatCompletion``` API와 지식창고 함수 간의 상호 작용을 지원하는 몇 가지 Python 함수를 포함하여 에이전트를 만들겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77a6fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, model=GPT_MODEL):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
    "    }\n",
    "    json_data = {\"model\": model, \"messages\": messages}\n",
    "    if functions is not None:\n",
    "        json_data.update({\"functions\": functions})\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"https://api.openai.com/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=json_data,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"Unable to generate ChatCompletion response\")\n",
    "        print(f\"Exception: {e}\")\n",
    "        return e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "73f7672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"function\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "978b7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate our get_articles and read_article_and_summarize functions\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"\"\"Use this function to get academic papers from arXiv to answer user questions.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            User query in JSON. Responses should be summarized and should include the article URL reference\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"\"\"Use this function to read whole papers and provide a summary for users.\n",
    "        You should NEVER call this function before get_articles has been called in the conversation.\"\"\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": f\"\"\"\n",
    "                            Description of the article in plain text based on the user's query\n",
    "                            \"\"\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c88ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"This function makes a ChatCompletion API call with the option of adding functions\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    full_message = response.json()[\"choices\"][0]\n",
    "    if full_message[\"finish_reason\"] == \"function_call\":\n",
    "        print(f\"Function generation requested, calling function\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(f\"Function not required, responding to user\")\n",
    "        return response.json()\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"Function calling function which executes function calls when the model believes it is necessary.\n",
    "    Currently extended by adding clauses to this if statement.\"\"\"\n",
    "\n",
    "    if full_message[\"message\"][\"function_call\"][\"name\"] == \"get_articles\":\n",
    "        try:\n",
    "            parsed_output = json.loads(\n",
    "                full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "            )\n",
    "            print(\"Getting search results\")\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(f\"Function execution failed\")\n",
    "            print(f\"Error message: {e}\")\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": full_message[\"message\"][\"function_call\"][\"name\"],\n",
    "                \"content\": str(results),\n",
    "            }\n",
    "        )\n",
    "        try:\n",
    "            print(\"Got search results, summarizing content\")\n",
    "            response = chat_completion_request(messages)\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"Function chat request failed\")\n",
    "\n",
    "    elif (\n",
    "        full_message[\"message\"][\"function_call\"][\"name\"] == \"read_article_and_summarize\"\n",
    "    ):\n",
    "        parsed_output = json.loads(\n",
    "            full_message[\"message\"][\"function_call\"][\"arguments\"]\n",
    "        )\n",
    "        print(\"Finding and reading paper\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Function does not exist and cannot be called\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3e7868",
   "metadata": {},
   "source": [
    "## arXiv 대화\n",
    "\n",
    "대화에서 함수를 테스트하여 이 모든 것을 종합해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c39a1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "253fd0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 21/21 [00:08<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "핵심 주장:\n",
       "- 이 논문은 심층 강화 학습의 맥락에서 클리핑된 대체 목적 함수를 사용하는 근접 정책 최적화 알고리즘 (PPO-Clip)의 이론적 분석에 초점을 맞추고 있다.\n",
       "- PPO-Clip의 전역 수렴 속도 보장을 신경 기반 함수 근사화에 대한 첫 번째 보장으로 제시하고자 한다.\n",
       "- PPO-Clip의 전역 수렴, 수렴 속도의 명시적 특성화, PPO-Clip의 재해석의 실험을 통한 검증 등이 이 논문의 주요 기여 사항이다.\n",
       "\n",
       "증거:\n",
       "- PPO-Clip의 분석에 대한 두 가지 기본적인 도전 과제를 식별하였다.\n",
       "- 저자들은 PPO-Clip의 클리핑 메커니즘으로 인해 각 정책 업데이트에 대한 단순한 닫힌 형식 표현의 부재와 신경 PPO-Clip의 장점 함수의 근사 오차와 클리핑 행동 간의 강한 결합을 지적하였다.\n",
       "- 이러한 도전 과제를 해결하기 위해 저자들은 두 가지 핵심 아이디어를 제안하였다.\n",
       "- 첫째로, 저자들은 PPO-Clip을 힌지 손실의 관점에서 재해석하여 정책 개선을 큰 마진 분류 문제의 해결과 연결하였다.\n",
       "- 둘째로, 저자들은 Entropic Mirror Descent Algorithm (EMDA)를 활용한 두 단계 정책 개선 체계를 제안하였다.\n",
       "- 이러한 프레임워크는 PPO-Clip의 정책 업데이트에 대한 다루기 쉬운 표현을 가능하게 하며, 클리핑 함수의 수렴에 대한 명시적인 특성화를 제공한다.\n",
       "\n",
       "결론:\n",
       "- 이 논문은 심층 강화 학습에서 PPO-Clip의 성능과 수렴 특성에 대한 이론적 통찰력을 제공하며, 중요한 미해결 질문에 대한 대답을 제시하고 PPO-Clip의 분석과 일반화를 위한 프레임워크를 제공한다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does PPO reinforcement learning work?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ca3e18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "핵심 주장:\n",
       "- 이 논문은 시퀀스 생성 작업에서 근접 정책 최적화(PPO)의 사용에 대해 논의한다.\n",
       "- PPO-dynamic이라는 동적 접근법을 제안하고, 시퀀스 생성 모델 최적화에 일반적으로 사용되는 정책 기울기와의 효과를 비교한다.\n",
       "- 결과는 PPO와 PPO-dynamic이 안정성과 성능 측면에서 정책 기울기를 능가한다는 것을 보여준다.\n",
       "- 논문은 또한 chit-chat 챗봇, 강화학습, 정책 기울기와 PPO의 텍스트 생성 작업에서의 사용에 대한 배경 정보를 제공한다.\n",
       "\n",
       "증거:\n",
       "- PPO-dynamic은 학습 곡선과 정확도 측면에서 다른 알고리즘보다 우수한 성능을 보인다.\n",
       "- PPO-dynamic은 REINFORCE와 MIXER와 비교했을 때 비슷한 성능을 보이며, 다양한 답변을 제공하는 능력을 보인다.\n",
       "- PPO-dynamic은 OpenSubtitles 데이터셋을 사용한 chit-chat 챗봇 작업에서 BLEU-2 점수를 사용하여 테스트되고 평가된다.\n",
       "- PPO-dynamic은 다른 최적화 알고리즘보다 약간 더 높은 BLEU-2 점수를 달성한다.\n",
       "- PPO와 PPO-dynamic의 학습 진행은 정책 기울기보다 안정적이며, PPO-dynamic은 빠르게 수렴한다.\n",
       "\n",
       "결론:\n",
       "- PPO는 시퀀스 학습에 더 나은 방법이며, REINFORCE를 사용하는 고분산 문제를 개선할 수 있다.\n",
       "- PPO-dynamic은 최적화 과정을 더욱 개선한다.\n",
       "\n",
       "논문의 핵심 내용:\n",
       "- KL 다이버전스를 계산하는 수학적 식(Equation 1)을 제시한다.\n",
       "- 이전 확률 분포와 새로운 확률 분포의 비율로 정의된 상수 β(x)를 소개한다(Equation 2).\n",
       "- KL 다이버전스를 계산하는 식(Equation 4)을 유도한다.\n",
       "- α(a)가 1보다 작다고 가정하고, ln(1 + α(a))와 ln(1 - α(a)Pold(a)/(1 - Pold(a)))를 근사화하는 Taylor 전개를 사용한다(Equation 5).\n",
       "- Equation 4의 항목을 Equation 5의 식으로 대체하여 KL 다이버전스의 단순화된 식(Equation 6)을 얻는다.\n",
       "- KL 다이버전스의 유도된 식(Equation 7)에 기반한 α(a)의 값 범위를 제시한다.\n",
       "- PPO와 PPO-dynamic 알고리즘의 의사 코드를 제공한다(Algorithm 1).\n",
       "- 기준선의 훈련과 다른 알고리즘에 사용된 하이퍼파라미터를 포함한 실험 설정을 설명한다.\n",
       "- REINFORCE와 PPO 방법에 의해 생성된 출력 분포를 비교하는 Figure 1을 제시한다.\n",
       "- REINFORCE와 PPO-dynamic 방법에 의해 생성된 출력 분포의 분산을 비교하는 Table 1을 제시한다.\n",
       "- REINFORCE, PPO, PPO-dynamic 알고리즘을 사용하여 훈련된 chit-chat 챗봇에 의해 생성된 출력 예시를 제시하는 Table 2를 제시한다.\n",
       "- 순환 신경망과 근접 정책 최적화 알고리즘에 대한 관련 연구에 대한 참고 자료를 포함한다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add another user message to induce our system to use the second tool\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"Can you read the PPO sequence generation paper for me and give me a summary\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "display(Markdown(updated_response[\"choices\"][0][\"message\"][\"content\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "91d653f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\\nYou summarize the papers clearly so the customer can decide which to read to answer their question.\\nYou always provide the article_url and title so the user can understand the name of the paper and click through to access it.\\nBegin!'},\n",
       " {'role': 'user', 'content': 'Hi, how does PPO reinforcement learning work?'},\n",
       " {'role': 'assistant',\n",
       "  'content': '핵심 주장:\\n- 이 논문은 심층 강화 학습의 맥락에서 클리핑된 대체 목적 함수를 사용하는 근접 정책 최적화 알고리즘 (PPO-Clip)의 이론적 분석에 초점을 맞추고 있다.\\n- PPO-Clip의 전역 수렴 속도 보장을 신경 기반 함수 근사화에 대한 첫 번째 보장으로 제시하고자 한다.\\n- PPO-Clip의 전역 수렴, 수렴 속도의 명시적 특성화, PPO-Clip의 재해석의 실험을 통한 검증 등이 이 논문의 주요 기여 사항이다.\\n\\n증거:\\n- PPO-Clip의 분석에 대한 두 가지 기본적인 도전 과제를 식별하였다.\\n- 저자들은 PPO-Clip의 클리핑 메커니즘으로 인해 각 정책 업데이트에 대한 단순한 닫힌 형식 표현의 부재와 신경 PPO-Clip의 장점 함수의 근사 오차와 클리핑 행동 간의 강한 결합을 지적하였다.\\n- 이러한 도전 과제를 해결하기 위해 저자들은 두 가지 핵심 아이디어를 제안하였다.\\n- 첫째로, 저자들은 PPO-Clip을 힌지 손실의 관점에서 재해석하여 정책 개선을 큰 마진 분류 문제의 해결과 연결하였다.\\n- 둘째로, 저자들은 Entropic Mirror Descent Algorithm (EMDA)를 활용한 두 단계 정책 개선 체계를 제안하였다.\\n- 이러한 프레임워크는 PPO-Clip의 정책 업데이트에 대한 다루기 쉬운 표현을 가능하게 하며, 클리핑 함수의 수렴에 대한 명시적인 특성화를 제공한다.\\n\\n결론:\\n- 이 논문은 심층 강화 학습에서 PPO-Clip의 성능과 수렴 특성에 대한 이론적 통찰력을 제공하며, 중요한 미해결 질문에 대한 대답을 제시하고 PPO-Clip의 분석과 일반화를 위한 프레임워크를 제공한다.'},\n",
       " {'role': 'user',\n",
       "  'content': 'Can you read the PPO sequence generation paper for me and give me a summary'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_conversation.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44682ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c9fac412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\\nYou summarize the papers clearly so the customer can decide which to read to answer their question.\\nYou always provide the article_url and title so the user can understand the name of the paper and click through to access it.\\nBegin!'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_conversation.conversation_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eae92f",
   "metadata": {},
   "source": [
    "## transformer 논문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b7b8b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a system message\n",
    "paper_system_message = \"\"\"You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\n",
    "You summarize the papers clearly so the customer can decide which to read to answer their question.\n",
    "You always provide the article_url and title so the user can understand the name of the paper and click through to access it.\n",
    "Begin!\"\"\"\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "93aa653e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'You are arXivGPT, a helpful assistant pulls academic papers to answer user questions.\\nYou summarize the papers clearly so the customer can decide which to read to answer their question.\\nYou always provide the article_url and title so the user can understand the name of the paper and click through to access it.\\nBegin!'}]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_conversation.conversation_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68c7fec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function generation requested, calling function\n",
      "Finding and reading paper\n",
      "Chunking text from paper\n",
      "Summarizing each chunk of text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 12/12 [00:06<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing into overall summary\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "핵심 주장:\n",
       "- 이 학술 논문은 Toolformer라는 언어 모델을 소개합니다. 이 모델은 간단한 API를 통해 외부 도구를 사용하는 방법을 스스로 학습할 수 있습니다.\n",
       "- Toolformer는 최신 정보에 접근하거나 정확한 계산을 수행하는 등 현재 언어 모델의 한계를 극복하기 위해 개발되었습니다.\n",
       "- Toolformer는 자기 지도 학습 방식으로 훈련되며, 각 API에 대해 몇 가지 데모만 필요합니다.\n",
       "- Toolformer는 계산기, 질의응답 시스템, 검색 엔진, 번역 시스템, 캘린더 등 다양한 도구를 포함하고 있습니다.\n",
       "- 실험 결과, Toolformer는 다양한 하위 작업에서 큰 모델과 경쟁력 있는 성능을 보여주며, 핵심 언어 모델 능력을 희생하지 않고 큰 모델보다 우수한 성능을 달성합니다.\n",
       "\n",
       "증거:\n",
       "- 이 논문은 큰 언어 모델을 사용하여 컨텍스트 학습을 통해 데이터셋을 생성하고 자기 지도 손실을 사용하여 유용한 API 호출을 결정하는 방법에 기반합니다.\n",
       "- Toolformer는 어떤 도구를 언제, 어떻게 사용할지 결정할 수 있으므로 특정 작업에 제한되지 않고 도구를 포괄적으로 사용할 수 있습니다.\n",
       "- 실험 결과, Toolformer는 도구 사용을 학습한 후 다양한 작업에서 큰 모델과 다른 기준선 모델보다 우수한 성능을 보입니다.\n",
       "\n",
       "결론:\n",
       "- Toolformer는 외부 도구를 사용하는 방법을 스스로 학습하는 언어 모델로, 다양한 작업에서 우수한 성능을 보입니다.\n",
       "- Toolformer는 현재 언어 모델의 한계를 극복하고 최신 정보에 접근하고 정확한 계산을 수행할 수 있도록 도와줍니다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add a user message\n",
    "paper_conversation.add_message(\"user\", \"Hi, how does Transformer work?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response[\"choices\"][0][\"message\"][\"content\"]\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2b43f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
